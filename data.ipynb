{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PODCAST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "- [Build Dataframe](#Build-dataframe)\n",
    "- [Episode Counts](#Episode-counts)\n",
    "- [Target Features](#Target-features)\n",
    "- [Analysis](#Analysis)\n",
    "- [Analysis Checkpoint 1](#Analysis-Checkpoint-1)\n",
    "- [Analysis Checkpoint 2](#Analysis-Checkpoint-2)\n",
    "- [Analysis Completed](#Analysis-completed)\n",
    "- [Machine Learning Preparation](#ML-preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spacy objects\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r Nightvale_df\n",
    "%store -r myDNA_df\n",
    "%store -r YWA_df\n",
    "%store -r uu_df\n",
    "%store -r radiolab_df\n",
    "%store -r tal_df\n",
    "%store -r bullseye\n",
    "%store -r mother\n",
    "%store -r hodgman\n",
    "%store -r flophouse\n",
    "%store -r switchblade\n",
    "%store -r mbmbam\n",
    "%store -r sawbones\n",
    "%store -r wonderful\n",
    "%store -r tgg\n",
    "%store -r ffire\n",
    "%store -r shmanners\n",
    "%store -r taz\n",
    "%store -r neoscum_df\n",
    "%store -r allusionist_df\n",
    "\n",
    "\n",
    "# %store -r freak_df\n",
    "# %store -r Lore_df\n",
    "# %store -r Invisible_df\n",
    "# %store -r OnBeing_df\n",
    "# %store -r StoryCorps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframe\n",
    "Read in each podcast's dataframes from spiders using jupyter notebook's %store -r function.\n",
    "Manually construct main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all podcasts\n",
    "data = pd.concat([Nightvale_df.reset_index(drop=True),\n",
    "                  myDNA_df.reset_index(drop=True),\n",
    "                  YWA_df.reset_index(drop=True),                  \n",
    "                  uu_df.reset_index(drop=True),\n",
    "                  radiolab_df.reset_index(drop=True),\n",
    "                  tal_df.reset_index(drop=True),\n",
    "                  bullseye,\n",
    "                  mother,\n",
    "                  hodgman,\n",
    "                  flophouse,\n",
    "                  switchblade,\n",
    "                  mbmbam,\n",
    "                  sawbones,\n",
    "                  wonderful,\n",
    "                  tgg,\n",
    "                  ffire,\n",
    "                  shmanners,\n",
    "                  taz,\n",
    "                  neoscum_df,\n",
    "                  allusionist_df],\n",
    "                  # StoryCorps_df, OnBeing_df, Invisible_df, Lore_df],\n",
    "                  keys = ['Welcome to Nightvale','Move Your DNA','You\\'re Wrong About','Unlocking Us',\n",
    "                         'Radiolab','This American Life', 'Bullseye with Jesse Thorn','One Bad Mother',\n",
    "                         'Judge John Hodgman','The Flophouse','Switchblade Sisters',\n",
    "                         'MBMBaM','Sawbones','Wonderful','The Greatest Generation','Friendly Fire','Shmanners',\n",
    "                          'The Adventure Zone','NeoScum', 'The Allusionist'], names=['podcast','#']).reset_index(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Podcast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shmanners</th>\n",
       "      <td>8</td>\n",
       "      <td>222</td>\n",
       "      <td>2020</td>\n",
       "      <td>ask shmanners idioms pt. 3</td>\n",
       "      <td>Shmanners 222: Ask Shmanners/Idioms Pt. 3 \\nPu...</td>\n",
       "      <td>shmanners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Move Your DNA</th>\n",
       "      <td>22</td>\n",
       "      <td>034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thoughts on Incontinence</td>\n",
       "      <td>[Episode 34: Thoughts on Incontinence, Â , Desc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Greatest Generation</th>\n",
       "      <td>5</td>\n",
       "      <td>322</td>\n",
       "      <td></td>\n",
       "      <td>final draft</td>\n",
       "      <td>Note: This show periodically replaces their ad...</td>\n",
       "      <td>the greatest generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>Gonads: X &amp; Y</td>\n",
       "      <td>GONADS: X AND Y FINAL WEB TRANSCRIPT    [ADVE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>509</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Rules</td>\n",
       "      <td>Prologue      Ira Glass  A few years back, w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>314</td>\n",
       "      <td>315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Parrot and the Potbellied Pig</td>\n",
       "      <td>Prologue      Ira Glass   When Rosie was a k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Allusionist</th>\n",
       "      <td>19</td>\n",
       "      <td>009</td>\n",
       "      <td>2015</td>\n",
       "      <td>the space between</td>\n",
       "      <td>visit  theallusionist.org/spaces  to find out ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>347</td>\n",
       "      <td>348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tough Room</td>\n",
       "      <td>Prologue      Ira Glass   They can laugh abo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes on Camp</td>\n",
       "      <td>Prologue      Ira Glass  It's a typical camp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>664</td>\n",
       "      <td>667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wartime Radio</td>\n",
       "      <td>Prologue: Prologue      Ira Glass   When Dav...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           # Episode  Year                              Title  \\\n",
       "podcast                                                                         \n",
       "Shmanners                  8     222  2020         ask shmanners idioms pt. 3   \n",
       "Move Your DNA             22     034   NaN           Thoughts on Incontinence   \n",
       "The Greatest Generation    5     322                              final draft   \n",
       "Radiolab                  18     NaN  2018                      Gonads: X & Y   \n",
       "This American Life       509     512   NaN                        House Rules   \n",
       "This American Life       314     315   NaN  The Parrot and the Potbellied Pig   \n",
       "The Allusionist           19     009  2015                  the space between   \n",
       "This American Life       347     348   NaN                         Tough Room   \n",
       "This American Life       108     109   NaN                      Notes on Camp   \n",
       "This American Life       664     667   NaN                      Wartime Radio   \n",
       "\n",
       "                                                                      Text  \\\n",
       "podcast                                                                      \n",
       "Shmanners                Shmanners 222: Ask Shmanners/Idioms Pt. 3 \\nPu...   \n",
       "Move Your DNA            [Episode 34: Thoughts on Incontinence, Â , Desc...   \n",
       "The Greatest Generation  Note: This show periodically replaces their ad...   \n",
       "Radiolab                  GONADS: X AND Y FINAL WEB TRANSCRIPT    [ADVE...   \n",
       "This American Life         Prologue      Ira Glass  A few years back, w...   \n",
       "This American Life         Prologue      Ira Glass   When Rosie was a k...   \n",
       "The Allusionist          visit  theallusionist.org/spaces  to find out ...   \n",
       "This American Life         Prologue      Ira Glass   They can laugh abo...   \n",
       "This American Life         Prologue      Ira Glass  It's a typical camp...   \n",
       "This American Life         Prologue: Prologue      Ira Glass   When Dav...   \n",
       "\n",
       "                                         Podcast  \n",
       "podcast                                           \n",
       "Shmanners                              shmanners  \n",
       "Move Your DNA                                NaN  \n",
       "The Greatest Generation  the greatest generation  \n",
       "Radiolab                                     NaN  \n",
       "This American Life                           NaN  \n",
       "This American Life                           NaN  \n",
       "The Allusionist                              NaN  \n",
       "This American Life                           NaN  \n",
       "This American Life                           NaN  \n",
       "This American Life                           NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "This American Life           734\n",
       "Radiolab                     261\n",
       "Welcome to Nightvale         180\n",
       "Move Your DNA                108\n",
       "The Allusionist               97\n",
       "Bullseye with Jesse Thorn     63\n",
       "MBMBaM                        32\n",
       "Sawbones                      30\n",
       "Wonderful                     29\n",
       "One Bad Mother                29\n",
       "Judge John Hodgman            28\n",
       "Shmanners                     28\n",
       "The Greatest Generation       28\n",
       "Friendly Fire                 28\n",
       "NeoScum                       20\n",
       "You're Wrong About            19\n",
       "The Adventure Zone            19\n",
       "The Flophouse                 16\n",
       "Switchblade Sisters           14\n",
       "Unlocking Us                  12\n",
       "Name: podcast, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)\n",
    "data.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Welcome to Nightvale</th>\n",
       "      <td>171</td>\n",
       "      <td>2020</td>\n",
       "      <td>go to the mirror</td>\n",
       "      <td>What makes you You? Welcome to Night Vale.  Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Later That Same Day</td>\n",
       "      <td>Prologue      Ira Glass   What can 20 years ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Secret Government</td>\n",
       "      <td>Prologue      Ira Glass  John Podesta used t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inside Job</td>\n",
       "      <td>Prologue      Ira Glass  A friend of mine ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>255</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Prologue      Ira Glass   One week before Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Episode  Year                Title  \\\n",
       "podcast                                                   \n",
       "Welcome to Nightvale     171  2020     go to the mirror   \n",
       "This American Life       243   NaN  Later That Same Day   \n",
       "This American Life       229   NaN    Secret Government   \n",
       "This American Life       405   NaN           Inside Job   \n",
       "This American Life       255   NaN                        \n",
       "\n",
       "                                                                   Text  \n",
       "podcast                                                                  \n",
       "Welcome to Nightvale  What makes you You? Welcome to Night Vale.  Do...  \n",
       "This American Life      Prologue      Ira Glass   What can 20 years ...  \n",
       "This American Life      Prologue      Ira Glass  John Podesta used t...  \n",
       "This American Life      Prologue      Ira Glass  A friend of mine ra...  \n",
       "This American Life      Prologue      Ira Glass   One week before Ch...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['#', 'Podcast'])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1775"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of texts less than 6500 characters\n",
    "podcast_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if len(data.iloc[i, 3]) > 6500:  # I kept changing this number to see what returned, this gets rid of the erroneous text\n",
    "        podcast_df = podcast_df.append(data.iloc[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(podcast_df.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>NaN</td>\n",
       "      <td>JAD ABUMRAD: Before we start, a quick heads...</td>\n",
       "      <td>In the No Part 2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>554</td>\n",
       "      <td>Prologue      Ira Glass  Adriana was talking...</td>\n",
       "      <td>Not It!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>370</td>\n",
       "      <td>Prologue      Ira Glass  There are people wh...</td>\n",
       "      <td>Ruining It for the Rest of Us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Welcome to Nightvale</th>\n",
       "      <td>010</td>\n",
       "      <td>Regret nothing, until it is too late. Then reg...</td>\n",
       "      <td>feral dogs</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeoScum</th>\n",
       "      <td>07</td>\n",
       "      <td>Mike Migdall (MM): It was really cool because,...</td>\n",
       "      <td>Walking the Edge</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shmanners</th>\n",
       "      <td>218</td>\n",
       "      <td>Shmanners 218: Animal Crossing \\nPublished Jul...</td>\n",
       "      <td>animal crossing</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sawbones</th>\n",
       "      <td>355</td>\n",
       "      <td>Sawbones 355: The Great Smog \\nPublished 2nd F...</td>\n",
       "      <td>the great smog</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>065</td>\n",
       "      <td>Passing      Ira Glass  From WBEZ Chicago, i...</td>\n",
       "      <td>Who's Canadian?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>182</td>\n",
       "      <td>Prologue      Ira Glass  Joe worked at this ...</td>\n",
       "      <td>Cringe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>643</td>\n",
       "      <td>Prologue      Ira Glass   A couple months ag...</td>\n",
       "      <td>Damned If You Doâ¦</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Episode  \\\n",
       "Radiolab                 NaN   \n",
       "This American Life       554   \n",
       "This American Life       370   \n",
       "Welcome to Nightvale     010   \n",
       "NeoScum                   07   \n",
       "Shmanners                218   \n",
       "Sawbones                 355   \n",
       "This American Life       065   \n",
       "This American Life       182   \n",
       "This American Life       643   \n",
       "\n",
       "                                                                   Text  \\\n",
       "Radiolab                 JAD ABUMRAD: Before we start, a quick heads...   \n",
       "This American Life      Prologue      Ira Glass  Adriana was talking...   \n",
       "This American Life      Prologue      Ira Glass  There are people wh...   \n",
       "Welcome to Nightvale  Regret nothing, until it is too late. Then reg...   \n",
       "NeoScum               Mike Migdall (MM): It was really cool because,...   \n",
       "Shmanners             Shmanners 218: Animal Crossing \\nPublished Jul...   \n",
       "Sawbones              Sawbones 355: The Great Smog \\nPublished 2nd F...   \n",
       "This American Life      Passing      Ira Glass  From WBEZ Chicago, i...   \n",
       "This American Life      Prologue      Ira Glass  Joe worked at this ...   \n",
       "This American Life      Prologue      Ira Glass   A couple months ag...   \n",
       "\n",
       "                                              Title  Year  \n",
       "Radiolab                           In the No Part 2  2018  \n",
       "This American Life                          Not It!   NaN  \n",
       "This American Life    Ruining It for the Rest of Us   NaN  \n",
       "Welcome to Nightvale                     feral dogs  2012  \n",
       "NeoScum                            Walking the Edge        \n",
       "Shmanners                           animal crossing  2020  \n",
       "Sawbones                             the great smog  2021  \n",
       "This American Life                  Who's Canadian?   NaN  \n",
       "This American Life                           Cringe   NaN  \n",
       "This American Life                Damned If You Doâ¦   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target features\n",
    "\n",
    "- Number of hosts: whole number = number of regular hosts, 0.5 represents if the podcast regularly has guests\n",
    "- genre (aka Tag 1)\n",
    "- topic (aka Tag 2)\n",
    "- scripted/unscripted\n",
    "- fiction/nonfiction\n",
    "- format: \"chat\" indicates general, unfocused conversation, and \"recap\" indicates specfic topic discussion\n",
    "- rating: from iTunes, range from 4.6 to 4.9\n",
    "\n",
    "This has been the hardest part of the project so far.  A lot of these categories are open to interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_feats = [['Welcome to Nightvale', 1, ['comedy', 'sci-fi'], 'scripted', 'fiction', 'news', 4.8],\n",
    "             ['Move Your DNA', 1.5, ['health', 'fitness'], 'unscripted', 'nonfiction', 'chat', 4.8],\n",
    "             ['You\\'re Wrong About', 2, ['history', 'education'], 'unscripted', 'nonfiction', 'chat', 4.6],\n",
    "             ['Unlocking Us', 1.5, ['health', 'lifestyle'], 'unscripted', 'nonfiction', 'interview', 4.6],\n",
    "             ['Radiolab', 2, ['society', 'education'], 'unscripted', 'nonfiction', 'storytelling', 4.7],\n",
    "             ['This American Life', 1.5, ['society','history'], 'unscripted', 'nonfiction', 'storytelling', 4.6],\n",
    "             ['Bullseye with Jesse Thorn' , 1.5, ['comedy', 'society'], 'unscripted', 'nonfiction', 'interview', 4.7],\n",
    "             ['One Bad Mother', 2.5, ['comedy', 'parenting'], 'unscripted', 'nonfiction', 'chat', 4.7],\n",
    "             ['Judge John Hodgman', 1.5, ['comedy, advice'], 'unscripted', 'nonfiction', 'chat', 4.8],\n",
    "             ['The Flophouse' , 3, ['comedy', 'movies'], 'unscripted', 'nonfiction', 'recap', 4.8],\n",
    "             ['Switchblade Sisters', 1.5, ['comedy', 'movies'], 'unscripted', 'nonfiction', 'chat', 4.9],\n",
    "             ['MBMBaM', 3, ['comedy','advice'], 'unscripted', 'nonfiction', 'chat', 4.9],\n",
    "             ['Sawbones', 2, ['history', 'medicine'], 'unscripted', 'nonfiction', 'storytelling', 4.8],\n",
    "             ['Wonderful', 2, ['comedy', 'society'], 'unscripted', 'nonfiction', 'chat', 4.9],\n",
    "             ['The Greatest Generation', 2, ['comedy', 'TV'], 'unscripted', 'nonfiction', 'recap', 4.9],\n",
    "             ['Friendly Fire', 3, ['history', 'movies'], 'unscripted', 'nonfiction', 'recap', 4.6],\n",
    "             ['Shmanners', 2, ['society', 'advice'], 'unscripted', 'nonfiction', 'chat', 4.8],\n",
    "             ['The Adventure Zone', 4, ['games', 'RP'], 'unscripted', 'fiction', 'LARP', 4.9],\n",
    "             ['NeoScum', 5, ['games', 'RP'], 'unscripted', 'fiction', 'LARP', 4.9],\n",
    "             ['The Allusionist', 1, ['education', 'language'], 'scripted', 'nonfiction','storytelling', 4.8]]\n",
    "\n",
    "# In case you're a cool person reading this and don't know, LARP is live action role playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Genre-Topic</th>\n",
       "      <th>Scripted/Un</th>\n",
       "      <th>Fiction/Non</th>\n",
       "      <th>Format</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Welcome to Nightvale</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[comedy, sci-fi]</td>\n",
       "      <td>scripted</td>\n",
       "      <td>fiction</td>\n",
       "      <td>news</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Move Your DNA</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[health, fitness]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unlocking Us</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[health, lifestyle]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>interview</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bullseye with Jesse Thorn</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[comedy, society]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>interview</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Bad Mother</th>\n",
       "      <td>2.5</td>\n",
       "      <td>[comedy, parenting]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge John Hodgman</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[comedy, advice]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Flophouse</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[comedy, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switchblade Sisters</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[comedy, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[comedy, advice]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sawbones</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, medicine]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wonderful</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[comedy, society]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Greatest Generation</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[comedy, TV]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shmanners</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, advice]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Adventure Zone</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[games, RP]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>fiction</td>\n",
       "      <td>LARP</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeoScum</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[games, RP]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>fiction</td>\n",
       "      <td>LARP</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Allusionist</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[education, language]</td>\n",
       "      <td>scripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Hosts            Genre-Topic Scripted/Un  \\\n",
       "podcast                                                               \n",
       "Welcome to Nightvale         1.0       [comedy, sci-fi]    scripted   \n",
       "Move Your DNA                1.5      [health, fitness]  unscripted   \n",
       "You're Wrong About           2.0   [history, education]  unscripted   \n",
       "Unlocking Us                 1.5    [health, lifestyle]  unscripted   \n",
       "Radiolab                     2.0   [society, education]  unscripted   \n",
       "This American Life           1.5     [society, history]  unscripted   \n",
       "Bullseye with Jesse Thorn    1.5      [comedy, society]  unscripted   \n",
       "One Bad Mother               2.5    [comedy, parenting]  unscripted   \n",
       "Judge John Hodgman           1.5       [comedy, advice]  unscripted   \n",
       "The Flophouse                3.0       [comedy, movies]  unscripted   \n",
       "Switchblade Sisters          1.5       [comedy, movies]  unscripted   \n",
       "MBMBaM                       3.0       [comedy, advice]  unscripted   \n",
       "Sawbones                     2.0    [history, medicine]  unscripted   \n",
       "Wonderful                    2.0      [comedy, society]  unscripted   \n",
       "The Greatest Generation      2.0           [comedy, TV]  unscripted   \n",
       "Friendly Fire                3.0      [history, movies]  unscripted   \n",
       "Shmanners                    2.0      [society, advice]  unscripted   \n",
       "The Adventure Zone           4.0            [games, RP]  unscripted   \n",
       "NeoScum                      5.0            [games, RP]  unscripted   \n",
       "The Allusionist              1.0  [education, language]    scripted   \n",
       "\n",
       "                          Fiction/Non        Format  Rating  \n",
       "podcast                                                      \n",
       "Welcome to Nightvale          fiction          news     4.8  \n",
       "Move Your DNA              nonfiction          chat     4.8  \n",
       "You're Wrong About         nonfiction          chat     4.6  \n",
       "Unlocking Us               nonfiction     interview     4.6  \n",
       "Radiolab                   nonfiction  storytelling     4.7  \n",
       "This American Life         nonfiction  storytelling     4.6  \n",
       "Bullseye with Jesse Thorn  nonfiction     interview     4.7  \n",
       "One Bad Mother             nonfiction          chat     4.7  \n",
       "Judge John Hodgman         nonfiction          chat     4.8  \n",
       "The Flophouse              nonfiction         recap     4.8  \n",
       "Switchblade Sisters        nonfiction          chat     4.9  \n",
       "MBMBaM                     nonfiction          chat     4.9  \n",
       "Sawbones                   nonfiction  storytelling     4.8  \n",
       "Wonderful                  nonfiction          chat     4.9  \n",
       "The Greatest Generation    nonfiction         recap     4.9  \n",
       "Friendly Fire              nonfiction         recap     4.6  \n",
       "Shmanners                  nonfiction          chat     4.8  \n",
       "The Adventure Zone            fiction          LARP     4.9  \n",
       "NeoScum                       fiction          LARP     4.9  \n",
       "The Allusionist            nonfiction  storytelling     4.8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pod_feats_df = pd.DataFrame(pod_feats, columns = ['podcast', 'Hosts', 'Genre-Topic', \n",
    "                                                  'Scripted/Un', 'Fiction/Non', \n",
    "                                                  'Format', 'Rating']).set_index('podcast')\n",
    "pod_feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(podcast_df.index.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Genre-Topic</th>\n",
       "      <th>Scripted/Un</th>\n",
       "      <th>Fiction/Non</th>\n",
       "      <th>Format</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td></td>\n",
       "      <td>Sarah Marshall  The internet is like a pipe fu...</td>\n",
       "      <td>Tipper Gore vs. Heavy Metal: The Case Against ...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Allusionist</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[education, language]</td>\n",
       "      <td>scripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.8</td>\n",
       "      <td>017</td>\n",
       "      <td>to hear this episode or read more about it, vi...</td>\n",
       "      <td>fix, part i</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>113</td>\n",
       "      <td>Prologue      Ira Glass   From WBEZ Chicago ...</td>\n",
       "      <td>Windfall</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>121</td>\n",
       "      <td>Prologue      Ira Glass  This story is alway...</td>\n",
       "      <td>Twentieth Century Man</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>242</td>\n",
       "      <td>Prologue      Ira Glass  Here's the story th...</td>\n",
       "      <td>Enemy Camp</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>468</td>\n",
       "      <td>Prologue      Ira Glass   This spring my fri...</td>\n",
       "      <td>Switcheroo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(SOUNDBITE OF MUSIC)   UNIDENTIFIED PERSON: L...</td>\n",
       "      <td>Deep Cuts</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>686</td>\n",
       "      <td>Prologue: Prologue      Ira Glass   It's nea...</td>\n",
       "      <td>Umbrellas Up</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>511</td>\n",
       "      <td>Prologue      Sarah Koenig  Can you just say...</td>\n",
       "      <td>The Seven Things Youâre Not Supposed to Talk A...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Allusionist</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[education, language]</td>\n",
       "      <td>scripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.8</td>\n",
       "      <td>061</td>\n",
       "      <td>visit  theallusionist.org/graphology  to read ...</td>\n",
       "      <td>in your hand</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Hosts            Genre-Topic Scripted/Un Fiction/Non  \\\n",
       "podcast                                                                    \n",
       "You're Wrong About    2.0   [history, education]  unscripted  nonfiction   \n",
       "The Allusionist       1.0  [education, language]    scripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "Radiolab              2.0   [society, education]  unscripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "This American Life    1.5     [society, history]  unscripted  nonfiction   \n",
       "The Allusionist       1.0  [education, language]    scripted  nonfiction   \n",
       "\n",
       "                          Format  Rating Episode  \\\n",
       "podcast                                            \n",
       "You're Wrong About          chat     4.6           \n",
       "The Allusionist     storytelling     4.8     017   \n",
       "This American Life  storytelling     4.6     113   \n",
       "This American Life  storytelling     4.6     121   \n",
       "This American Life  storytelling     4.6     242   \n",
       "This American Life  storytelling     4.6     468   \n",
       "Radiolab            storytelling     4.7     NaN   \n",
       "This American Life  storytelling     4.6     686   \n",
       "This American Life  storytelling     4.6     511   \n",
       "The Allusionist     storytelling     4.8     061   \n",
       "\n",
       "                                                                 Text  \\\n",
       "podcast                                                                 \n",
       "You're Wrong About  Sarah Marshall  The internet is like a pipe fu...   \n",
       "The Allusionist     to hear this episode or read more about it, vi...   \n",
       "This American Life    Prologue      Ira Glass   From WBEZ Chicago ...   \n",
       "This American Life    Prologue      Ira Glass  This story is alway...   \n",
       "This American Life    Prologue      Ira Glass  Here's the story th...   \n",
       "This American Life    Prologue      Ira Glass   This spring my fri...   \n",
       "Radiolab             (SOUNDBITE OF MUSIC)   UNIDENTIFIED PERSON: L...   \n",
       "This American Life    Prologue: Prologue      Ira Glass   It's nea...   \n",
       "This American Life    Prologue      Sarah Koenig  Can you just say...   \n",
       "The Allusionist     visit  theallusionist.org/graphology  to read ...   \n",
       "\n",
       "                                                                Title  Year  \n",
       "podcast                                                                      \n",
       "You're Wrong About  Tipper Gore vs. Heavy Metal: The Case Against ...  2021  \n",
       "The Allusionist                                           fix, part i  2015  \n",
       "This American Life                                           Windfall   NaN  \n",
       "This American Life                              Twentieth Century Man   NaN  \n",
       "This American Life                                         Enemy Camp   NaN  \n",
       "This American Life                                         Switcheroo   NaN  \n",
       "Radiolab                                                    Deep Cuts  2021  \n",
       "This American Life                                       Umbrellas Up   NaN  \n",
       "This American Life  The Seven Things Youâre Not Supposed to Talk A...   NaN  \n",
       "The Allusionist                                          in your hand  2017  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df = pod_feats_df.join(podcast_df, on='podcast', sort=True)\n",
    "podcast_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1443"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(podcast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.616"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anticipating a log of ugly floats\n",
    "def percent(decimal):\n",
    "    decimal *= 100\n",
    "    percentage = '{:.3f}'.format(decimal)\n",
    "    percentage = float(percentage)\n",
    "    return percentage\n",
    "\n",
    "percent(0.45615981981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for t in podcast_df.Text:\n",
    "    if isinstance(t, float):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df = podcast_df[podcast_df['Text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(podcast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here, I added about 50 non-lexical features.  Almost all of them use custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Tokens column\n",
    "podcast_df['Tokens'] = podcast_df.Text.map(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And now, the news. Have any of our listeners seen the glowing cloud that has been moving in from the west? Well, John Peters, you know, the farmer? He saw it over the Western Ridge this morning, said he would have thought it was the setting sun if it wasnât for the time of day. Apparently the cloud glows in a variety of colors, perhaps changing from observer to observer, although all report a low whistling when it draws near. One death has already been attributed to the glowcloud.  But listen, itâs probably nothing. If we had to shut down the town for every mysterious event that at least one death could be attributed to, weâd never have time to do anything, right? Thatâs what the Sheriffâs Secret Police are saying, and I agree, although I would not go so far as to endorse their suggestion to ârun directly at the cloud, shrieking and waving your arms, just to see what it does.â  The Apache Tracker, and I remind you that this is that white guy who wears the huge and cartoonishly inaccurate Indian headdress, has announced that he has found some disturbing evidence concerning the recent incident at the Night Vale Post Office, which has been sealed by the City Council since the great screaming that was heard from it a few weeks ago. He said that using ancient Indian magicks, he slipped through Council security into the Post Office and observed that all the letters and packages had been thrown about as in a whirlwind, that there was the heavy stench of scorched flesh, and that words written in blood on the wall said âMore to comeâ¦and soon.â Can you believe this guy said he used âIndian Magicksâ? What an asshole.  Hereâs something odd: There is a cat hovering in the menâs bathroom at the radio station here. Seems perfectly happy and healthy, but itâs floating about four feet off the ground next to the sink. Doesnât seem to be able to move from its current hoverspot. If you pet him, he purrs, and heâll rub on your body like a normal cat if you get close enough. Fortunately, because heâs right by the sink, it was pretty easy to leave some water and food where he could get it, and itâs nice to have a station pet. Wish it wasnât trapped in a hovering prison in the menâs bathroom, but listen, no pet is perfect. It becomes perfect when you learn to accept it for what it is.  And now, a"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.Tokens.loc['Welcome to Nightvale'][2][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top50(Tokens):\n",
    "    counts = Counter(t.text for t in Tokens if t.is_alpha)\n",
    "    return counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Top50 column\n",
    "podcast_df['Top50'] = podcast_df.Tokens.map(top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy)\n",
    "- top50 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add token count (transcript_length) column\n",
    "podcast_df['Token_count'] = podcast_df.Tokens.map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy)\n",
    "- top50 (50 most common tokens)\n",
    "- Token_count (transcript length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_len(Tokens):\n",
    "    if len(Tokens) > 10:\n",
    "        lengths = [(w, len(w.text)) for w in Tokens if w.is_alpha]\n",
    "    else:\n",
    "        lengths = [('null',0)]\n",
    "    \n",
    "    avg = statistics.mean([l[-1] for l in lengths])\n",
    "    \n",
    "    return lengths, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add token length column\n",
    "podcast_df['Token_lengths'] = podcast_df.Tokens.map(lambda x: word_len(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average token length column\n",
    "podcast_df['Avg_token_len'] = podcast_df.Tokens.map(lambda x: word_len(x)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens  (spacy)\n",
    "- top50  (50 most common tokens)\n",
    "- Token_count  (transcript length)\n",
    "- Token_lengths  (list of tuples: (token, length))\n",
    "- Avg_token_len  (float, mean of all alphabetic token lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTR\n",
    "import random\n",
    "\n",
    "def get_ttr(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        lower = [t.text.lower() for t in Tokens if t.is_alpha]\n",
    "        rand = random.randint(0, len(lower))\n",
    "        chunk = lower[rand:(rand + 300)]\n",
    "        ttr = percent(len(set(lower))/len(lower))\n",
    "    else:\n",
    "        ttr = 0\n",
    "        \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TTR column\n",
    "podcast_df['TTR'] = podcast_df.Tokens.map(get_ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy)\n",
    "- top50 (50 most common tokens)\n",
    "- Token_count (transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (type/token ratio measured against 300 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in k-bands\n",
    "import pickle\n",
    "f = open('data/goog_kband.pkl','rb')\n",
    "goog_kband = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "goog_kband['throughout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kband(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        kbands = []\n",
    "        for t in Tokens:\n",
    "            if t.lemma_ in goog_kband:\n",
    "                kbands.append((t, goog_kband[t.lemma_]))\n",
    "        avg_kband = statistics.mean([t[1] for t in kbands])\n",
    "    else:\n",
    "        kbands = 0\n",
    "        avg_kband = 0\n",
    "    \n",
    "    return kbands, avg_kband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ('word', kband) column\n",
    "podcast_df['kband'] = podcast_df.Tokens.map(lambda x: get_kband(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average kband column\n",
    "podcast_df['Avg_kband'] = podcast_df.Tokens.map(lambda x: get_kband(x)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        bigrams = []\n",
    "        for t in Tokens[:-1]:\n",
    "            if t.text.isalpha() and Tokens[t.i + 1].text.isalpha():\n",
    "                bigram = (t.text.lower(), Tokens[t.i + 1].text.lower())\n",
    "                bigrams.append(bigram)\n",
    "        counts = Counter(b for b in bigrams).most_common(25)\n",
    "    else:\n",
    "        bigrams = 'null'\n",
    "        \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add bigrams column\n",
    "podcast_df['Bigrams'] = podcast_df.Tokens.map(lambda x: bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 25 most common bigram column\n",
    "podcast_df['Bigram_top25'] = podcast_df.Bigrams.map(lambda x: Counter(x).most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add (token, part-of-speech) column\n",
    "podcast_df['POS'] = podcast_df.Tokens.map(lambda t: [(w, w.pos_) for w in t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighs pos frequency against total text length\n",
    "def POS_frequency(POS_text):\n",
    "    counts = Counter(elem[-1].upper() for elem in POS_text)\n",
    "    total = len(POS_text)\n",
    "    \n",
    "    pos_freq = {}\n",
    "    for (pos, count) in counts.items():\n",
    "        pos_freq[pos] = percent(count/total)\n",
    "        \n",
    "    return pos_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add {part-of-speech: frequency} column\n",
    "podcast_df['POS_freq'] = podcast_df.POS.map(POS_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPACE': 14.54,\n",
       " 'VERB': 9.559,\n",
       " 'PUNCT': 14.211,\n",
       " 'DET': 7.671,\n",
       " 'NOUN': 12.356,\n",
       " 'ADV': 3.604,\n",
       " 'PRON': 7.704,\n",
       " 'ADP': 6.239,\n",
       " 'ADJ': 3.348,\n",
       " 'SCONJ': 1.036,\n",
       " 'CCONJ': 2.2,\n",
       " 'AUX': 3.359,\n",
       " 'NUM': 3.053,\n",
       " 'PROPN': 6.557,\n",
       " 'X': 0.785,\n",
       " 'PART': 1.638,\n",
       " 'INTJ': 2.139}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.POS_freq[1]\n",
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add part-of-speech frequency columns\n",
    "podcast_df['Noun_freq'] = podcast_df.POS_freq.map(lambda x: x.get('NOUN', 'null'))\n",
    "podcast_df['Proper_noun_freq'] = podcast_df.POS_freq.map(lambda x: x.get('PROPN', 'null'))\n",
    "podcast_df['Verb_freq'] = podcast_df.POS_freq.map(lambda x: x.get('VERB', 'null'))\n",
    "podcast_df['Adj_freq'] = podcast_df.POS_freq.map(lambda x: x.get('ADJ', 'null'))\n",
    "podcast_df['Adv_freq'] = podcast_df.POS_freq.map(lambda x: x.get('ADV', 'null'))\n",
    "podcast_df['Interjection_freq'] = podcast_df.POS_freq.map(lambda x: x.get('INTJ', 'null'))\n",
    "podcast_df['Preposition_freq'] = podcast_df.POS_freq.map(lambda x: x.get('ADP', 'null'))\n",
    "podcast_df['Conjunction_freq'] = podcast_df.POS_freq.map(lambda x: x.get('SCONJ', 'null'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Checkpoint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Genre-Topic</th>\n",
       "      <th>Scripted/Un</th>\n",
       "      <th>Fiction/Non</th>\n",
       "      <th>Format</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>...</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_freq</th>\n",
       "      <th>Noun_freq</th>\n",
       "      <th>Proper_noun_freq</th>\n",
       "      <th>Verb_freq</th>\n",
       "      <th>Adj_freq</th>\n",
       "      <th>Adv_freq</th>\n",
       "      <th>Interjection_freq</th>\n",
       "      <th>Preposition_freq</th>\n",
       "      <th>Conjunction_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>354</td>\n",
       "      <td>Prologue      Ira Glass   OK, this just in. ...</td>\n",
       "      <td>Mistakes Were Made</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[(  , SPACE), (Prologue, NOUN), (     , SPACE)...</td>\n",
       "      <td>{'SPACE': 3.289, 'NOUN': 11.861, 'PROPN': 7.03...</td>\n",
       "      <td>11.861</td>\n",
       "      <td>7.030</td>\n",
       "      <td>12.552</td>\n",
       "      <td>4.352</td>\n",
       "      <td>6.037</td>\n",
       "      <td>0.886</td>\n",
       "      <td>7.535</td>\n",
       "      <td>1.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JA: Jad Abumrad  SA: Simon Adler  AM: Annie M...</td>\n",
       "      <td>The Curious Case of the Russian Flash Mob ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>[( , SPACE), (JA, PROPN), (:, PUNCT), (Jad, PR...</td>\n",
       "      <td>{'SPACE': 4.55, 'PROPN': 5.918, 'PUNCT': 13.03...</td>\n",
       "      <td>13.292</td>\n",
       "      <td>5.918</td>\n",
       "      <td>12.385</td>\n",
       "      <td>4.307</td>\n",
       "      <td>5.023</td>\n",
       "      <td>2.224</td>\n",
       "      <td>7.771</td>\n",
       "      <td>1.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[RADIOLAB INTRO]    PAT WALTERS: Jad?    JAD ...</td>\n",
       "      <td>Dispatches from 1918</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>[( , SPACE), ([, PUNCT), (RADIOLAB, PROPN), (I...</td>\n",
       "      <td>{'SPACE': 4.183, 'PUNCT': 18.314, 'PROPN': 9.5...</td>\n",
       "      <td>12.351</td>\n",
       "      <td>9.528</td>\n",
       "      <td>9.364</td>\n",
       "      <td>3.914</td>\n",
       "      <td>5.076</td>\n",
       "      <td>1.957</td>\n",
       "      <td>7.768</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>664</td>\n",
       "      <td>Prologue: Prologue      Ira Glass   A man wa...</td>\n",
       "      <td>The Room of Requirement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[(  , SPACE), (Prologue, NOUN), (:, PUNCT), (P...</td>\n",
       "      <td>{'SPACE': 5.659, 'NOUN': 12.68, 'PUNCT': 12.47...</td>\n",
       "      <td>12.680</td>\n",
       "      <td>8.391</td>\n",
       "      <td>10.574</td>\n",
       "      <td>4.219</td>\n",
       "      <td>5.899</td>\n",
       "      <td>0.813</td>\n",
       "      <td>8.577</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>253</td>\n",
       "      <td>Prologue      Ira Glass   It took a week to ...</td>\n",
       "      <td>The Middle of Nowhere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[(  , SPACE), (Prologue, NOUN), (     , SPACE)...</td>\n",
       "      <td>{'SPACE': 3.639, 'NOUN': 13.448, 'PROPN': 7.63...</td>\n",
       "      <td>13.448</td>\n",
       "      <td>7.634</td>\n",
       "      <td>11.893</td>\n",
       "      <td>4.152</td>\n",
       "      <td>5.864</td>\n",
       "      <td>0.877</td>\n",
       "      <td>8.668</td>\n",
       "      <td>1.340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Hosts           Genre-Topic Scripted/Un Fiction/Non  \\\n",
       "podcast                                                                   \n",
       "This American Life    1.5    [society, history]  unscripted  nonfiction   \n",
       "Radiolab              2.0  [society, education]  unscripted  nonfiction   \n",
       "Radiolab              2.0  [society, education]  unscripted  nonfiction   \n",
       "This American Life    1.5    [society, history]  unscripted  nonfiction   \n",
       "This American Life    1.5    [society, history]  unscripted  nonfiction   \n",
       "\n",
       "                          Format  Rating Episode  \\\n",
       "podcast                                            \n",
       "This American Life  storytelling     4.6     354   \n",
       "Radiolab            storytelling     4.7     NaN   \n",
       "Radiolab            storytelling     4.7     NaN   \n",
       "This American Life  storytelling     4.6     664   \n",
       "This American Life  storytelling     4.6     253   \n",
       "\n",
       "                                                                 Text  \\\n",
       "podcast                                                                 \n",
       "This American Life    Prologue      Ira Glass   OK, this just in. ...   \n",
       "Radiolab             JA: Jad Abumrad  SA: Simon Adler  AM: Annie M...   \n",
       "Radiolab             [RADIOLAB INTRO]    PAT WALTERS: Jad?    JAD ...   \n",
       "This American Life    Prologue: Prologue      Ira Glass   A man wa...   \n",
       "This American Life    Prologue      Ira Glass   It took a week to ...   \n",
       "\n",
       "                                                                Title  Year  \\\n",
       "podcast                                                                       \n",
       "This American Life                                 Mistakes Were Made   NaN   \n",
       "Radiolab                The Curious Case of the Russian Flash Mob ...  2018   \n",
       "Radiolab                                         Dispatches from 1918  2020   \n",
       "This American Life                            The Room of Requirement   NaN   \n",
       "This American Life                              The Middle of Nowhere   NaN   \n",
       "\n",
       "                    ...                                                POS  \\\n",
       "podcast             ...                                                      \n",
       "This American Life  ...  [(  , SPACE), (Prologue, NOUN), (     , SPACE)...   \n",
       "Radiolab            ...  [( , SPACE), (JA, PROPN), (:, PUNCT), (Jad, PR...   \n",
       "Radiolab            ...  [( , SPACE), ([, PUNCT), (RADIOLAB, PROPN), (I...   \n",
       "This American Life  ...  [(  , SPACE), (Prologue, NOUN), (:, PUNCT), (P...   \n",
       "This American Life  ...  [(  , SPACE), (Prologue, NOUN), (     , SPACE)...   \n",
       "\n",
       "                                                             POS_freq  \\\n",
       "podcast                                                                 \n",
       "This American Life  {'SPACE': 3.289, 'NOUN': 11.861, 'PROPN': 7.03...   \n",
       "Radiolab            {'SPACE': 4.55, 'PROPN': 5.918, 'PUNCT': 13.03...   \n",
       "Radiolab            {'SPACE': 4.183, 'PUNCT': 18.314, 'PROPN': 9.5...   \n",
       "This American Life  {'SPACE': 5.659, 'NOUN': 12.68, 'PUNCT': 12.47...   \n",
       "This American Life  {'SPACE': 3.639, 'NOUN': 13.448, 'PROPN': 7.63...   \n",
       "\n",
       "                    Noun_freq Proper_noun_freq  Verb_freq  Adj_freq Adv_freq  \\\n",
       "podcast                                                                        \n",
       "This American Life     11.861            7.030     12.552     4.352    6.037   \n",
       "Radiolab               13.292            5.918     12.385     4.307    5.023   \n",
       "Radiolab               12.351            9.528      9.364     3.914    5.076   \n",
       "This American Life     12.680            8.391     10.574     4.219    5.899   \n",
       "This American Life     13.448            7.634     11.893     4.152    5.864   \n",
       "\n",
       "                    Interjection_freq Preposition_freq Conjunction_freq  \n",
       "podcast                                                                  \n",
       "This American Life              0.886            7.535            1.268  \n",
       "Radiolab                        2.224            7.771            1.086  \n",
       "Radiolab                        1.957            7.768            0.978  \n",
       "This American Life              0.813            8.577            0.960  \n",
       "This American Life              0.877            8.668            1.340  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_length(POS_text):\n",
    "    pos_dict = {'NOUN': 0, 'VERB': 0, 'ADV': 0, 'ADJ': 0}\n",
    "    pron_dict = {'i': 0, 'you': 0, 'she': 0, 'he': 0, 'it': 0, 'they': 0, 'we': 0}\n",
    "    for (token, pos) in POS_text:\n",
    "        if pos in pos_dict.keys():\n",
    "            pos_dict[pos] = (pos_dict[pos] + len(token.text))/2\n",
    "        if token.text in pron_dict.keys():\n",
    "            pron_dict[token.text] = pron_dict[token.text] + 1\n",
    "    \n",
    "    if sum(pron_dict.values()) != 0:\n",
    "        pron_total = sum(pron_dict.values())\n",
    "    \n",
    "    if sum(pron_dict.values()) != 0:\n",
    "        for (p, c) in pron_dict.items():\n",
    "            pron_dict[p] = percent(c/pron_total)\n",
    "    \n",
    "    \n",
    "    return pos_dict, pron_dict\n",
    "\n",
    "# Average word length of each POS\n",
    "# POS_length[0][0] = noun\n",
    "#           [0][1] = verb\n",
    "#           [0][2] = adv\n",
    "#           [0][3] = adj\n",
    "\n",
    "# Individual pronoun occurrence weighed against total # of pronouns\n",
    "# POS_length[1][1] = 'i'\n",
    "#           [1][2] = 'you'\n",
    "#           [1][3] ='she'\n",
    "#           [1][4] = 'he'\n",
    "#           [1][5] = 'it'\n",
    "#           [1][6] = 'they'\n",
    "#           [1][7] = 'we'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['POS_length'] = podcast_df.POS.map(lambda p: POS_length(p)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['Avg_noun_len'] = podcast_df.POS_length.map(lambda d: d['NOUN'])\n",
    "podcast_df['Avg_verb_len'] = podcast_df.POS_length.map(lambda d: d['VERB'])\n",
    "podcast_df['Avg_adj_len'] = podcast_df.POS_length.map(lambda d: d['ADJ'])\n",
    "podcast_df['Avg_adv_len'] = podcast_df.POS_length.map(lambda d: d['ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['Pron_counts'] = podcast_df.POS.map(lambda p: POS_length(p)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['i_count'] = podcast_df.Pron_counts.map(lambda d: d['i'])\n",
    "podcast_df['you_count'] = podcast_df.Pron_counts.map(lambda d: d['you'])\n",
    "podcast_df['she_count'] = podcast_df.Pron_counts.map(lambda d: d['she'])\n",
    "podcast_df['he_count'] = podcast_df.Pron_counts.map(lambda d: d['he'])\n",
    "podcast_df['it_count'] = podcast_df.Pron_counts.map(lambda d: d['it'])\n",
    "podcast_df['they_count'] = podcast_df.Pron_counts.map(lambda d: d['they'])\n",
    "podcast_df['we_count'] = podcast_df.Pron_counts.map(lambda d: d['we'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPACE': 13.087,\n",
       " 'VERB': 10.211,\n",
       " 'PUNCT': 13.748,\n",
       " 'DET': 7.533,\n",
       " 'NOUN': 11.726,\n",
       " 'ADV': 4.144,\n",
       " 'PRON': 9.205,\n",
       " 'ADP': 6.561,\n",
       " 'ADJ': 3.428,\n",
       " 'SCONJ': 1.184,\n",
       " 'CCONJ': 2.545,\n",
       " 'AUX': 3.705,\n",
       " 'NUM': 2.457,\n",
       " 'PROPN': 5.564,\n",
       " 'INTJ': 2.087,\n",
       " 'PART': 2.057,\n",
       " 'X': 0.735,\n",
       " 'SYM': 0.025}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.726"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.POS_freq[0]\n",
    "podcast_df.Noun_freq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'â': 9.848,\n",
       " 'be': 5.594,\n",
       " 'know': 4.604,\n",
       " 'have': 4.196,\n",
       " 'think': 3.322,\n",
       " 'get': 3.147,\n",
       " 'go': 2.972,\n",
       " 'laugh': 2.739,\n",
       " 'do': 2.681,\n",
       " 'make': 2.564,\n",
       " 'see': 2.273,\n",
       " 'say': 1.573,\n",
       " 'feel': 1.573,\n",
       " 'gon': 1.34,\n",
       " 'chuckle': 1.224,\n",
       " 'watch': 1.224,\n",
       " 'take': 1.049,\n",
       " 'want': 1.049,\n",
       " 'love': 1.049,\n",
       " 'mean': 0.991}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common verb lemmas\n",
    "def verb_lemmas(POS_text):\n",
    "    counts = Counter(elem[0].lemma_ for elem in POS_text if elem[1] == 'VERB')\n",
    "    \n",
    "    verb_counter = {}\n",
    "    for (verb, value) in counts.most_common(20):\n",
    "        verb_counter[verb] = percent(value/sum(counts.values()))\n",
    "        \n",
    "    return verb_counter\n",
    "\n",
    "verb_lemmas(podcast_df.POS[1])\n",
    "# spacy thinks that an apostrophe is a verb?  Wonder why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add verb_lemmas column\n",
    "podcast_df['verb_lemmas'] = podcast_df.POS.map(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sent_toks column\n",
    "podcast_df['Sent_toks'] = podcast_df.Text.map(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor alteration to unit_len\n",
    "def sent_len(doc):\n",
    "    sentlens = []\n",
    "    for c in doc:\n",
    "        length = len([l for l in c.split()])\n",
    "        sentlens.append((c, length))\n",
    "        \n",
    "    return sentlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  \\nNote: This show periodically replaces their ad breaks with new promotional clips.',\n",
       "  12),\n",
       " ('Because of this, both the \\ntranscription for the clips and the timestamps after them may be inaccurate at the time of viewing this \\ntranscript.',\n",
       "  24),\n",
       " ('00:00:00  Music  Music  âService and Deployment,â composed by Mark Isham, from the \\nalbum Megan Leavey (Original Motion Picture Soundtrack) plays as \\nJohn speaks.',\n",
       "  23),\n",
       " ('It is a minimalist, ethereal synth melody.', 7),\n",
       " ('00:00:01  John  Host  Now this is a dog movie, so a certain percentage of our audience \\nRoderick  has already decided itâs a 5âMilk-Bone film or whatever without even \\nwatching it.',\n",
       "  30),\n",
       " ('The dog people, you know the ones I mean, the \\nAnubisians.', 11),\n",
       " ('When I was a kid, dogs roamed around outside doing \\ndog things like shitting everywhere and licking their peanuts and \\nhumping each other and severely biting kids on the leg that were \\nonly trying to ride their bikes to the Northway Mall and who made the \\nmistake of riding through Mountain View instead of going out Debarr \\nRoad, God only knows why.',\n",
       "  62),\n",
       " ('No one held dogs in high esteem.', 7),\n",
       " ('Dogs were just things in the \\nworld, like crows and garbage cans and El Caminos and drainage \\nditches that led to culverts under the street where the light filtered \\ndown through the holes in the manhole covers, and where you knew \\nbetter than to light firecrackers because your mom was convinced \\nthe sewers were full of methane and youâd blow up the \\nneighborhood.',\n",
       "  63),\n",
       " (\"Dogs weren't important.\", 3)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len(podcast_df.Sent_toks[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['Sent_length'] = podcast_df.Sent_toks.map(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['Avg_sent_len'] = podcast_df.Sent_length.map(lambda s: statistics.mean([t[-1] for t in s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Checkpoint 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})\n",
    "- Sent_toks  (list of sentences)\n",
    "- Sent_length  (list of tuples as (sentence, sentence length))\n",
    "- Avg_sent_len  (float, average sentence length over entire transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WORK_OF_ART': 0.025,\n",
       " 'PERSON': 3.039,\n",
       " 'CARDINAL': 0.197,\n",
       " 'FAC': 0.03,\n",
       " 'LOC': 0.02,\n",
       " 'ORDINAL': 0.039,\n",
       " 'DATE': 0.207,\n",
       " 'ORG': 0.197,\n",
       " 'PRODUCT': 0.054,\n",
       " 'GPE': 0.168,\n",
       " 'EVENT': 0.03,\n",
       " 'MONEY': 0.271,\n",
       " 'NORP': 0.113,\n",
       " 'PERCENT': 0.03,\n",
       " 'TIME': 0.025,\n",
       " 'QUANTITY': 0.01}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add entity frequency column\n",
    "def ent_counter(doc):\n",
    "    ents = []    \n",
    "    people = []\n",
    "    length = len(doc)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        ents.append((ent.text, ent.label_))\n",
    "        if ent.label_ == 'PERSON':\n",
    "            people.append(ent.text.lower().strip())\n",
    "    count = dict(Counter([ent[1] for ent in ents]))\n",
    "    for label, c in count.items():\n",
    "        count[label] = percent(c/length)\n",
    "        \n",
    "    return count, people\n",
    "\n",
    "ent_counter(podcast_df.Tokens[0])[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRODUCT': 0.014,\n",
       " 'PERSON': 3.019,\n",
       " 'ORG': 0.258,\n",
       " 'NORP': 0.167,\n",
       " 'CARDINAL': 0.324,\n",
       " 'GPE': 0.176,\n",
       " 'ORDINAL': 0.062,\n",
       " 'MONEY': 0.21,\n",
       " 'EVENT': 0.043,\n",
       " 'DATE': 0.172,\n",
       " 'TIME': 0.014,\n",
       " 'WORK_OF_ART': 0.038,\n",
       " 'LANGUAGE': 0.005,\n",
       " 'LOC': 0.019,\n",
       " 'FAC': 0.01,\n",
       " 'QUANTITY': 0.024}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Ents'] = podcast_df.Tokens.map(lambda x: (ent_counter(x)[0], ent_counter(x)[-1]))\n",
    "podcast_df.Ents[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUES ARE % OF ENTIRE PODCAST LENGTH\n",
    "podcast_df['Organization'] = podcast_df.Ents.map(lambda x: x[0].get('ORG', 0))\n",
    "podcast_df['Art'] = podcast_df.Ents.map(lambda x: x[0].get('WORK_OF_ART', 0))\n",
    "podcast_df['Date'] = podcast_df.Ents.map(lambda x: x[0].get('DATE', 0))\n",
    "podcast_df['Geopolitical'] = podcast_df.Ents.map(lambda x: x[0].get('GPE', 0))\n",
    "podcast_df['Numbers'] = podcast_df.Ents.map(lambda x: x[0].get('CARDINAL', 0))\n",
    "podcast_df['Event'] = podcast_df.Ents.map(lambda x: x[0].get('EVENT', 0))\n",
    "podcast_df['Cash'] = podcast_df.Ents.map(lambda x: x[0].get('MONEY', 0))\n",
    "podcast_df['Time'] = podcast_df.Ents.map(lambda x: x[0].get('TIME', 0))\n",
    "podcast_df['Product'] = podcast_df.Ents.map(lambda x: x[0].get('PRODUCT', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why no host recognition?\n",
    "I originally wanted to try and do something with parsing out host names then singling out their speech.  This turned out to be a bit of a pipe dream, since A. spacy isn't very good at recognizing names in the somewhat chaotic transcripts and B. there's no way of differentiating between a speaker's tag (i.e. Jad Abumrad: talk talk talk) and a name just being mentioned in speech.  Also formatting is wildly different both from podcast to podcast and within the same podcasts.  For instance, Radiolab formats speaker tag four different ways (JA:, Jad, JAD, JAD ABUMRAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# my beautiful and *definitely* state-of-the-art spacy matcher pattern-maker.  I found it really annoying to have to\n",
    "#      format a pattern matcher every time I wanted to look for something new\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def pattern_maker():\n",
    "    \n",
    "    patterns = []\n",
    "    add_items = True\n",
    "    add_dict = True    \n",
    "    \n",
    "    while add_dict:        \n",
    "        match_single = {}                    \n",
    "            \n",
    "        while add_items:       \n",
    "            tag = input('enter tag (lowercase):  ').upper()\n",
    "            if tag == '0':\n",
    "                add_dict = False\n",
    "                break\n",
    "\n",
    "            string = input('enter string (all lowercase):  ')\n",
    "            if string == 'true' or string == 'false':\n",
    "                string = bool(string)\n",
    "\n",
    "            if tag == 'POS':\n",
    "                string = string.upper()\n",
    "            \n",
    "            match_single[tag] = string\n",
    "            \n",
    "            add_items = input('add more to this dict?(y/n)  ')\n",
    "            if add_items == 'n':\n",
    "                patterns.append(match_single)\n",
    "                break\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if len(patterns) == 0:\n",
    "        return\n",
    "   \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " '.',\n",
       " ',',\n",
       " '.',\n",
       " 'â',\n",
       " ',',\n",
       " 'â',\n",
       " ',',\n",
       " '(',\n",
       " ')',\n",
       " '.',\n",
       " ',',\n",
       " '.',\n",
       " ',',\n",
       " 'â',\n",
       " '-',\n",
       " '.',\n",
       " ',',\n",
       " ',',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pattern_matcher(pattern, doc): \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('pattern', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    \n",
    "    match_strings = []\n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end].text\n",
    "        match_strings.append(matched_span)\n",
    "    # print('{} matches found'.format(len(match_strings)))\n",
    "    return match_strings\n",
    "\n",
    "pattern_matcher([{'IS_PUNCT': True}], podcast_df.Tokens[0])[:20]\n",
    "# sorry about the giant flash of punctuation, I'm not sure why it's even printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({':': 52,\n",
       "         '.': 752,\n",
       "         'â': 89,\n",
       "         'â': 90,\n",
       "         '(': 6,\n",
       "         ')': 3,\n",
       "         '-': 76,\n",
       "         ';': 5,\n",
       "         '?': 133,\n",
       "         'â': 165,\n",
       "         '!': 186,\n",
       "         '[': 126,\n",
       "         ']': 126,\n",
       "         '&': 1,\n",
       "         'â¦': 47,\n",
       "         'â': 10,\n",
       "         'â': 6,\n",
       "         '\"': 1,\n",
       "         '%': 2,\n",
       "         '):': 3})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Punctuation'] = podcast_df.Tokens.map(lambda t: Counter([p for p in pattern_matcher([{'IS_PUNCT': True}], t) if p != ',']))\n",
    "podcast_df.Punctuation[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_freq</th>\n",
       "      <th>excl_freq</th>\n",
       "      <th>quest_freq</th>\n",
       "      <th>hyph_freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>35.132</td>\n",
       "      <td>5.795</td>\n",
       "      <td>6.115</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>33.926</td>\n",
       "      <td>8.562</td>\n",
       "      <td>6.422</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>35.208</td>\n",
       "      <td>6.587</td>\n",
       "      <td>5.971</td>\n",
       "      <td>0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>35.177</td>\n",
       "      <td>6.525</td>\n",
       "      <td>5.234</td>\n",
       "      <td>1.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>35.253</td>\n",
       "      <td>6.605</td>\n",
       "      <td>5.298</td>\n",
       "      <td>1.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>32.447</td>\n",
       "      <td>6.844</td>\n",
       "      <td>5.219</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>29.617</td>\n",
       "      <td>2.973</td>\n",
       "      <td>4.868</td>\n",
       "      <td>2.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>31.822</td>\n",
       "      <td>3.050</td>\n",
       "      <td>4.334</td>\n",
       "      <td>3.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>30.610</td>\n",
       "      <td>3.612</td>\n",
       "      <td>4.348</td>\n",
       "      <td>2.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>31.569</td>\n",
       "      <td>4.714</td>\n",
       "      <td>4.946</td>\n",
       "      <td>3.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>30.678</td>\n",
       "      <td>3.605</td>\n",
       "      <td>4.975</td>\n",
       "      <td>2.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>28.902</td>\n",
       "      <td>4.178</td>\n",
       "      <td>5.145</td>\n",
       "      <td>2.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>28.322</td>\n",
       "      <td>5.031</td>\n",
       "      <td>5.060</td>\n",
       "      <td>2.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>29.642</td>\n",
       "      <td>3.979</td>\n",
       "      <td>5.637</td>\n",
       "      <td>2.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>28.064</td>\n",
       "      <td>5.207</td>\n",
       "      <td>4.802</td>\n",
       "      <td>2.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>31.257</td>\n",
       "      <td>3.933</td>\n",
       "      <td>5.047</td>\n",
       "      <td>2.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>33.989</td>\n",
       "      <td>4.038</td>\n",
       "      <td>5.688</td>\n",
       "      <td>1.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>28.781</td>\n",
       "      <td>4.835</td>\n",
       "      <td>4.704</td>\n",
       "      <td>2.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>28.831</td>\n",
       "      <td>4.864</td>\n",
       "      <td>5.808</td>\n",
       "      <td>1.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBMBaM</th>\n",
       "      <td>29.883</td>\n",
       "      <td>4.661</td>\n",
       "      <td>5.038</td>\n",
       "      <td>2.330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         period_freq  excl_freq  quest_freq  hyph_freq\n",
       "podcast                                               \n",
       "MBMBaM        35.132      5.795       6.115      0.520\n",
       "MBMBaM        33.926      8.562       6.422      0.889\n",
       "MBMBaM        35.208      6.587       5.971      0.886\n",
       "MBMBaM        35.177      6.525       5.234      1.391\n",
       "MBMBaM        35.253      6.605       5.298      1.378\n",
       "MBMBaM        32.447      6.844       5.219      0.739\n",
       "MBMBaM        29.617      2.973       4.868      2.861\n",
       "MBMBaM        31.822      3.050       4.334      3.491\n",
       "MBMBaM        30.610      3.612       4.348      2.665\n",
       "MBMBaM        31.569      4.714       4.946      3.053\n",
       "MBMBaM        30.678      3.605       4.975      2.812\n",
       "MBMBaM        28.902      4.178       5.145      2.762\n",
       "MBMBaM        28.322      5.031       5.060      2.704\n",
       "MBMBaM        29.642      3.979       5.637      2.089\n",
       "MBMBaM        28.064      5.207       4.802      2.339\n",
       "MBMBaM        31.257      3.933       5.047      2.193\n",
       "MBMBaM        33.989      4.038       5.688      1.896\n",
       "MBMBaM        28.781      4.835       4.704      2.418\n",
       "MBMBaM        28.831      4.864       5.808      1.784\n",
       "MBMBaM        29.883      4.661       5.038      2.330"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['period_freq'] = podcast_df.Punctuation.map(lambda d: percent(d['.']/sum(d.values())))\n",
    "podcast_df['excl_freq'] = podcast_df.Punctuation.map(lambda d: percent(d['!']/sum(d.values())))\n",
    "podcast_df['quest_freq'] = podcast_df.Punctuation.map(lambda d: percent(d['?']/sum(d.values())))\n",
    "podcast_df['hyph_freq'] = podcast_df.Punctuation.map(lambda d: percent(d['-']/sum(d.values())))\n",
    "podcast_df.loc['MBMBaM', 'period_freq':'hyph_freq'][:20]\n",
    "# numbers are percentage of all punctuation except commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Hosts', 'Genre-Topic', 'Scripted/Un', 'Fiction/Non', 'Format',\n",
       "       'Rating', 'Episode', 'Text', 'Title', 'Year', 'Tokens', 'Top50',\n",
       "       'Token_count', 'Token_lengths', 'Avg_token_len', 'TTR', 'kband',\n",
       "       'Avg_kband', 'Bigrams', 'Bigram_top25', 'POS', 'POS_freq', 'Noun_freq',\n",
       "       'Proper_noun_freq', 'Verb_freq', 'Adj_freq', 'Adv_freq',\n",
       "       'Interjection_freq', 'Preposition_freq', 'Conjunction_freq',\n",
       "       'POS_length', 'Avg_noun_len', 'Avg_verb_len', 'Avg_adj_len',\n",
       "       'Avg_adv_len', 'Pron_counts', 'i_count', 'you_count', 'she_count',\n",
       "       'he_count', 'it_count', 'they_count', 'we_count', 'verb_lemmas',\n",
       "       'Sent_toks', 'Sent_length', 'Avg_sent_len', 'Ents', 'Organization',\n",
       "       'Art', 'Date', 'Geopolitical', 'Numbers', 'Event', 'Cash', 'Time',\n",
       "       'Product', 'Punctuation', 'period_freq', 'excl_freq', 'quest_freq',\n",
       "       'hyph_freq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})\n",
    "- Sent_toks  (list of sentences)\n",
    "- Sent_length  (list of tuples as (sentence, sentence length))\n",
    "- Avg_sent_len  (float, average sentence length over entire transcript)\n",
    "- Ents  (dictionary as {spacy's ent tag of token: % of ent occurrence over document length})\n",
    "- Organization  (float, % of organization entities against doc length)\n",
    "- Art  (float, % of art entities against doc length)\n",
    "- Date  (float, % of date entities against doc length)\n",
    "- Geopolitical  (float, % of geopolitical entities (countries, cities, etc.) against doc length)\n",
    "- Numbers  (float, % of number occurrence against doc length)\n",
    "- Event  (float, % of event entity occurrence against doc length)\n",
    "- Cash  (float, % of monetary value entity occurrence against doc length)\n",
    "- Time  (float, % of time entity tags occurrence against doc length)\n",
    "- Product  (float, % of product entity tag occurrence against doc length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['phone rings', 'weather: âLemonade in the Shadeâ by   ']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a regex for tags\n",
    "expression = r'(?<=\\[).+?(?=\\])'\n",
    "len(re.findall(expression, podcast_df.Text[100]))\n",
    "re.findall(expression, podcast_df.loc['Welcome to Nightvale', 'Text'][0])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Background music fades into podcast theme.',\n",
       " 'Drumroll begins, which leads into the theme song.',\n",
       " 'Song fades down and plays quietly as host begins to speak.',\n",
       " 'John groans and Ben chuckles.',\n",
       " 'Laughs deliberately',\n",
       " 'Adam makes a thoughtful sound and Ben laughs loudly.',\n",
       " 'John laughs.',\n",
       " 'Chuckles',\n",
       " 'Ben laughs loudly.',\n",
       " 'Ben laughs.',\n",
       " 'Somberly',\n",
       " 'chuckles',\n",
       " 'sighs',\n",
       " 'Laughs.',\n",
       " 'Adam laughs.',\n",
       " 'John laughs, Ben starts chuckling.',\n",
       " 'John chuckles.',\n",
       " 'John laughs.',\n",
       " 'chuckles',\n",
       " 'Chuckles',\n",
       " 'Adam makes a couple of affirming sounds as Adam speaks.',\n",
       " 'Adam chuckles.',\n",
       " 'Ben makes a couple of affirming sounds as John speaks.',\n",
       " 'Ben laughs.',\n",
       " 'Ben and John laughs.',\n",
       " 'chuckles',\n",
       " 'Adam chuckles briefly.',\n",
       " 'Adam laughs.',\n",
       " 'Laughs.',\n",
       " 'Chuckling, amused',\n",
       " 'Chuckling',\n",
       " 'Laughs',\n",
       " 'Ben laughs heartily.',\n",
       " 'Ben laughs heavily.',\n",
       " 'chuckling',\n",
       " 'Laughs and claps in the background.',\n",
       " 'imitates a teenagerâs voice',\n",
       " 'A score of minimal, slightly unearthly, emotional synth music plays.',\n",
       " 'stutters',\n",
       " 'Thoughtfully',\n",
       " 'Ben makes a few affirming sounds as John speaks.',\n",
       " 'In a high-pitched, restrained voice',\n",
       " 'chuckles briefly',\n",
       " 'Chuckles',\n",
       " 'imitates a Bronx accent',\n",
       " 'Also in a Bronx accent',\n",
       " 'Still in accent',\n",
       " 'Still in accent',\n",
       " 'Ben laughs.',\n",
       " 'A crowd cheering, with a soft, uplifting brass film score underneath.',\n",
       " 'Chuckles.',\n",
       " 'Surprised and pleased',\n",
       " 'Adam chuckles.',\n",
       " 'Adam giggles.',\n",
       " 'Chuckling',\n",
       " 'chuckles',\n",
       " 'John laughs pointedly.',\n",
       " 'Ben laughs.',\n",
       " 'Laughs',\n",
       " 'laughing',\n",
       " 'Chuckling',\n",
       " 'Ben chuckles.',\n",
       " 'Intrigued',\n",
       " 'Quietly',\n",
       " 'chuckles',\n",
       " 'Someone chuckles.',\n",
       " 'Ben cracks up.',\n",
       " 'Deadpan',\n",
       " 'Morse Code begins to play as Ben finishes the IMDB page quote.',\n",
       " 'Chuckles',\n",
       " 'Adam laughs.',\n",
       " 'John chuckles.',\n",
       " 'Under his breath',\n",
       " 'Laughs uproariously, then, catching his breath',\n",
       " 'Quietly',\n",
       " 'John laughs.',\n",
       " 'Ben laughs.',\n",
       " 'Ben laughs loudly.',\n",
       " 'inaudible',\n",
       " 'John chuckles.',\n",
       " 'breaks off, laughing',\n",
       " 'John and Ben make thoughtful sounds.',\n",
       " 'Ben chuckles.',\n",
       " 'Adam chuckles.',\n",
       " 'Adam makes a thoughtful sound.',\n",
       " 'Adam chuckles.',\n",
       " 'Adam makes an affirming sound.',\n",
       " 'chuckles',\n",
       " 'Chuckles',\n",
       " 'Ben chuckles an affirming sound.',\n",
       " 'Laughs heartily',\n",
       " 'Chuckles.',\n",
       " 'Chuckling',\n",
       " 'Whispers',\n",
       " 'Regular tone',\n",
       " 'Quietly',\n",
       " 'John chuckles.',\n",
       " 'Ben laughs.',\n",
       " 'Adam laughs.',\n",
       " 'Adam makes a thoughtful sound.',\n",
       " 'Ben laughs.',\n",
       " 'Adam laughs.',\n",
       " 'Adam chuckles.',\n",
       " 'Chuckling',\n",
       " 'Ben laughs.',\n",
       " 'Ben chuckles.',\n",
       " 'makes a disgusted sound',\n",
       " 'Ben chuckles.',\n",
       " 'Laughs.',\n",
       " 'Chuckles.',\n",
       " 'Ben laughs',\n",
       " 'John laughs.',\n",
       " 'Stutters',\n",
       " 'Ben makes a thoughtful sound.',\n",
       " 'Adam chuckles briefly.',\n",
       " 'Someone makes a thoughtful sound.',\n",
       " 'Beat',\n",
       " 'Chuckles',\n",
       " 'Chuckles',\n",
       " 'Theme song plays for a while at full volume before fading out.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe get len and parts of speech within tags\n",
    "podcast_df['Tags'] = podcast_df.Text.map(lambda text: re.findall(expression, text))\n",
    "len(podcast_df.Tags[0])\n",
    "podcast_df.Tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "def avg_tag_length(tag_list):\n",
    "    lengths = []\n",
    "       \n",
    "    for t in tag_list:\n",
    "        t = t.split(' ')\n",
    "        lengths.append(len(t))\n",
    "       \n",
    "    if len(lengths) > 0:\n",
    "        avg_length = statistics.mean(lengths)\n",
    "    else:\n",
    "        avg_length = 0\n",
    "    \n",
    "    \n",
    "    return avg_length\n",
    "        \n",
    "avg_tag_length(podcast_df.Tags[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "podcast\n",
       "Friendly Fire    2.900000\n",
       "Friendly Fire    2.664179\n",
       "Friendly Fire    2.658730\n",
       "Friendly Fire    2.385542\n",
       "Friendly Fire    3.036364\n",
       "Friendly Fire    2.712230\n",
       "Friendly Fire    2.737374\n",
       "Friendly Fire    2.000000\n",
       "Friendly Fire    2.206186\n",
       "Friendly Fire    2.151515\n",
       "Friendly Fire    2.368852\n",
       "Friendly Fire    2.288288\n",
       "Friendly Fire    2.348837\n",
       "Friendly Fire    2.180952\n",
       "Friendly Fire    2.364583\n",
       "Friendly Fire    2.425532\n",
       "Friendly Fire    2.364865\n",
       "Friendly Fire    2.384615\n",
       "Friendly Fire    2.481928\n",
       "Friendly Fire    2.607477\n",
       "Name: Tag_len, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Tag_len'] = podcast_df.Tags.map(avg_tag_length)\n",
    "podcast_df.Tag_len[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chuckle'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_top_verb(tag_text_list):\n",
    "    lemmas = []\n",
    "    \n",
    "    for t in tag_text_list:\n",
    "        t = nlp(t)\n",
    "        for token in t:\n",
    "            if token.pos_=='VERB':\n",
    "                lemmas.append(token.lemma_)    \n",
    "    \n",
    "    if len(lemmas) > 0:\n",
    "        top_verb = Counter(lemmas).most_common(1)[0][0]\n",
    "    else:\n",
    "        top_verb = 'NaN'\n",
    "        \n",
    "    return top_verb\n",
    "        \n",
    "        \n",
    "tag_top_verb(podcast_df.Tags[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crosstalk'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Tag_top_verb'] = podcast_df.Tags.map(tag_top_verb)\n",
    "podcast_df.Tag_top_verb[115]\n",
    "# including this column definitely reflects confirmation bias, since I think that \n",
    "#      having \"laugh\" in a tag will weight it in favor of being comedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})\n",
    "- Sent_toks  (list of sentences)\n",
    "- Sent_length  (list of tuples as (sentence, sentence length))\n",
    "- Avg_sent_len  (float, average sentence length over entire transcript)\n",
    "- Ents  (dictionary as {spacy's ent tag of token: % of ent occurrence over document length})\n",
    "- Organization  (float, % of organization entities against doc length)\n",
    "- Art  (float, % of art entities against doc length)\n",
    "- Date  (float, % of date entities against doc length)\n",
    "- Geopolitical  (float, % of geopolitical entities (countries, cities, etc.) against doc length)\n",
    "- Numbers  (float, % of number occurrence against doc length)\n",
    "- Event  (float, % of event entity occurrence against doc length)\n",
    "- Cash  (float, % of monetary value entity occurrence against doc length)\n",
    "- Time  (float, % of time entity tags occurrence against doc length)\n",
    "- Product  (float, % of product entity tag occurrence against doc length)\n",
    "- Tag_len  (float, average word length of text tags (i.e. [he laughed], [music plays]))\n",
    "- Top_tag_verb  (string, most commonly-occurring verb lemma within tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I apologize in advance to sensitive eyes and my parents\n",
    "swears = ['fuck', 'fucking','fucker', 'shit','ass','asshole','damn','dammit', 'goddamnn','bitch','bitchy','cunt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "podcast\n",
       "Wonderful             0.131\n",
       "You're Wrong About    0.137\n",
       "You're Wrong About    0.098\n",
       "You're Wrong About    0.021\n",
       "You're Wrong About    0.130\n",
       "You're Wrong About    0.146\n",
       "You're Wrong About    0.055\n",
       "You're Wrong About    0.131\n",
       "You're Wrong About    0.056\n",
       "You're Wrong About    0.120\n",
       "Name: Swear_count, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Swear_count'] = podcast_df.Tokens.map(lambda tokens: percent(len([t.text for t in tokens if t.text in swears])/len(tokens)))\n",
    "podcast_df.Swear_count[-20:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "podcast\n",
       "Wonderful             0.028\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.009\n",
       "You're Wrong About    0.006\n",
       "You're Wrong About    0.000\n",
       "You're Wrong About    0.000\n",
       "Name: Fake_swear_count, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_swears = ['fudge','shoot','butthead','darn']\n",
    "podcast_df['Fake_swear_count'] = podcast_df.Tokens.map(lambda tokens: percent(len([t.text for t in tokens if t.text in fake_swears])/len(tokens)))\n",
    "podcast_df.Fake_swear_count[-20:-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns so far:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})\n",
    "- Sent_toks  (list of sentences)\n",
    "- Sent_length  (list of tuples as (sentence, sentence length))\n",
    "- Avg_sent_len  (float, average sentence length over entire transcript)\n",
    "- Ents  (dictionary as {spacy's ent tag of token: % of ent occurrence over document length})\n",
    "- Organization  (float, % of organization entities against doc length)\n",
    "- Art  (float, % of art entities against doc length)\n",
    "- Date  (float, % of date entities against doc length)\n",
    "- Geopolitical  (float, % of geopolitical entities (countries, cities, etc.) against doc length)\n",
    "- Numbers  (float, % of number occurrence against doc length)\n",
    "- Event  (float, % of event entity occurrence against doc length)\n",
    "- Cash  (float, % of monetary value entity occurrence against doc length)\n",
    "- Time  (float, % of time entity tags occurrence against doc length)\n",
    "- Product  (float, % of product entity tag occurrence against doc length)\n",
    "- Tag_len  (float, average word length of text tags (i.e. [he laughed], [music plays]))\n",
    "- Top_tag_verb  (string, most commonly-occurring verb lemma within tags)\n",
    "- Swear_count  (float, % of text tokens that are swear words)\n",
    "- Fake_swear_count  (float, % of text tokens that are fake swear words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I felt', 'you feel']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['I thought']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'I thought that there are three more of I than you.  I felt good about this.  Do you feel okay?'\n",
    "pattern_matcher([{'POS': 'PRON'}, {'POS': 'AUX', 'OP': '*'}, {'LEMMA': 'feel'}], nlp(string))\n",
    "pattern_matcher([{'POS': 'PRON'}, {'POS': 'AUX', 'OP': '*'}, {'LEMMA': 'think'}], nlp(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def opinions(Tokens):\n",
    "    count = 0\n",
    "    \n",
    "    count += len(pattern_matcher([{'POS': 'PRON'}, {'POS': 'AUX', 'OP': '*'}, {'LEMMA': 'feel'}], Tokens))\n",
    "    count += len(pattern_matcher([{'POS': 'PRON'}, {'POS': 'AUX', 'OP': '*'}, {'LEMMA': 'think'}], nlp(string)))\n",
    "    \n",
    "    verb_count = len([t for t in Tokens if t.pos_ == 'VERB'])\n",
    "    count /= verb_count\n",
    "    return count\n",
    "\n",
    "opinions(nlp(string))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027070925825663237"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Opinion_count'] = podcast_df.Tokens.map(opinions)\n",
    "podcast_df.Opinion_count[449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_per_sent(POSes, sent_toks):\n",
    "    POSes = len([x[1] for x in POSes if x[1] == 'ADP'])\n",
    "    adps = POSes/len(sent_toks)\n",
    "    \n",
    "    return adps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "podcast\n",
       "Friendly Fire    1.230342\n",
       "Friendly Fire    1.159420\n",
       "Friendly Fire    1.281599\n",
       "Friendly Fire    1.381510\n",
       "Friendly Fire    1.198267\n",
       "                   ...   \n",
       "Radiolab         0.943898\n",
       "Radiolab         0.854637\n",
       "Radiolab         0.950197\n",
       "Radiolab         1.079086\n",
       "Radiolab         0.716392\n",
       "Name: Prep_per_sent, Length: 157, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Prep_per_sent'] = podcast_df.apply(lambda x: prep_per_sent(POSes = x['POS'], sent_toks = x['Sent_toks']), axis=1)\n",
    "podcast_df.Prep_per_sent[:157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1380\n",
       "1      45\n",
       "2       6\n",
       "3       5\n",
       "Name: Donation_appeal, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Donation_appeal'] = podcast_df.Tokens.map(lambda t: len(pattern_matcher([{'LEMMA':'donate', 'DEP': 'ROOT'}], t)))\n",
    "podcast_df.Donation_appeal.value_counts()\n",
    "# this doesn't look like it'll be particularly useful, but I made it so it can't hurt to leave it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = ['twitter','facebook','instagram','linkedin','twitch','tik tok']\n",
    "\n",
    "def social_count(tokens):\n",
    "    count = 0\n",
    "    for t in tokens:\n",
    "        if t.text in sm:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1359\n",
       "1      30\n",
       "2      24\n",
       "3      21\n",
       "4       2\n",
       "Name: Social, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df['Social'] = podcast_df.Tokens.map(social_count)\n",
    "podcast_df.Social.value_counts()\n",
    "# same as last column: doesn't seem like it'd be particularly useful, but might be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Hosts', 'Genre-Topic', 'Scripted/Un', 'Fiction/Non', 'Format',\n",
       "       'Rating', 'Episode', 'Text', 'Title', 'Year', 'Tokens', 'Top50',\n",
       "       'Token_count', 'Token_lengths', 'Avg_token_len', 'TTR', 'kband',\n",
       "       'Avg_kband', 'Bigrams', 'Bigram_top25', 'POS', 'POS_freq', 'Noun_freq',\n",
       "       'Proper_noun_freq', 'Verb_freq', 'Adj_freq', 'Adv_freq',\n",
       "       'Interjection_freq', 'Preposition_freq', 'Conjunction_freq',\n",
       "       'POS_length', 'Avg_noun_len', 'Avg_verb_len', 'Avg_adj_len',\n",
       "       'Avg_adv_len', 'Pron_counts', 'i_count', 'you_count', 'she_count',\n",
       "       'he_count', 'it_count', 'they_count', 'we_count', 'verb_lemmas',\n",
       "       'Sent_toks', 'Sent_length', 'Avg_sent_len', 'Ents', 'Organization',\n",
       "       'Art', 'Date', 'Geopolitical', 'Numbers', 'Event', 'Cash', 'Time',\n",
       "       'Product', 'Punctuation', 'period_freq', 'excl_freq', 'quest_freq',\n",
       "       'hyph_freq', 'Tags', 'Tag_len', 'Tag_top_verb', 'Swear_count',\n",
       "       'Fake_swear_count', 'Opinion_count', 'Prep_per_sent', 'Donation_appeal',\n",
       "       'Social'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df['Know'] = podcast_df.verb_lemmas.map(lambda x: x.get('know', 0))\n",
    "podcast_df['Be'] = podcast_df.verb_lemmas.map(lambda x: x.get('be', 0))\n",
    "podcast_df['Do'] = podcast_df.verb_lemmas.map(lambda x: x.get('do', 0))\n",
    "podcast_df['Mean'] = podcast_df.verb_lemmas.map(lambda x: x.get('mean', 0))\n",
    "podcast_df['Make'] = podcast_df.verb_lemmas.map(lambda x: x.get('make', 0))\n",
    "podcast_df['Go'] = podcast_df.verb_lemmas.map(lambda x: x.get('go', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all features:\n",
    "- Tokens (spacy doc)\n",
    "- top50 (Counter of 50 most common tokens and their counts)\n",
    "- Token_count (int, transcript length)\n",
    "- Token_lengths (list of tuples: (token, length))\n",
    "- Avg_token_len (float, mean of all alphabetic token lengths)\n",
    "- TTR  (float, type/token ratio measured against 300 characters)\n",
    "- kband  (list of tuples as (word, kband))\n",
    "- Avg_kband  (float, mean kband)\n",
    "- Bigrams (list of bigram tuples)\n",
    "- Bigram_top25  (Counter object of 25 most common bigrams and their counts)\n",
    "- POS  (list of tuples as (token, spacy POS tag))\n",
    "- POS_freq  (dictionary as {POS: % of entire document})\n",
    "- Noun_freq  (float, % of tokens that are nouns)\n",
    "- Verb_freq  (float, % of tokens that are verbs)\n",
    "- Adj_freq  (float, % of tokens that are adjectives)\n",
    "- Adv_freq  (float, % of tokens that are adverbs)\n",
    "- Interjection_freq  (float, % of tokens that are interjections)\n",
    "- Preposition_freq  (float, % of tokens that are prepositions)\n",
    "- Conjunction_freq  (float, % of tokens that are conjunctions)\n",
    "- POS_length  (dictionary as {POS: average character length})\n",
    "- Avg_noun_len  (float, average character length of all nouns)\n",
    "- Avg_verb_len  (float, average character length of all verbs)\n",
    "- Avg_adj_len  (float, average character length of all adjectives)\n",
    "- Avg_adv_len  (float, average character length of all adverbs)\n",
    "- Pron_counts  (dictionary as {pronoun: % of all pronoun occurrence that this pronoun makes up})\n",
    "- i_count  (float, % of pronouns that are 'i')\n",
    "- you_count  (float, % of pronouns that are 'you')\n",
    "- she_count  (float, % of pronouns that are 'she')\n",
    "- he_count  (float, % of pronouns that are 'he')\n",
    "- it_count  (float, % of pronouns that are 'it')\n",
    "- they_count (float, % of pronouns that are 'they')\n",
    "- we_count  (float, % of pronouns that are 'we')\n",
    "- verb_lemmas  (dictionary of 20 most common verb lemmas as {lemma: % of all verbs that verb comprises})\n",
    "- Sent_toks  (list of sentences)\n",
    "- Sent_length  (list of tuples as (sentence, sentence length))\n",
    "- Avg_sent_len  (float, average sentence length over entire transcript)\n",
    "- Ents  (dictionary as {spacy's ent tag of token: % of ent occurrence over document length})\n",
    "- Organization  (float, % of organization entities against doc length)\n",
    "- Art  (float, % of art entities against doc length)\n",
    "- Date  (float, % of date entities against doc length)\n",
    "- Geopolitical  (float, % of geopolitical entities (countries, cities, etc.) against doc length)\n",
    "- Numbers  (float, % of number occurrence against doc length)\n",
    "- Event  (float, % of event entity occurrence against doc length)\n",
    "- Cash  (float, % of monetary value entity occurrence against doc length)\n",
    "- Time  (float, % of time entity tags occurrence against doc length)\n",
    "- Product  (float, % of product entity tag occurrence against doc length)\n",
    "- Tag_len  (float, average word length of text tags (i.e. [he laughed], [music plays]))\n",
    "- Top_tag_verb  (string, most commonly-occurring verb lemma within tags)\n",
    "- Swear_count  (float, % of text tokens that are swear words)\n",
    "- Fake_swear_count  (float, % of text tokens that are fake swear words)\n",
    "- Opinion_count  (float, occurrence of pronoun followed by optional auxiliary followed by lemma think or feel weighed\n",
    "                    against total verb occurrence)\n",
    "- Prep_per_sent  (float, average occurrence of prepositions per sentence)\n",
    "- Donation_appeal  (int, count of \"donate\" occurring as a phrase root)\n",
    "- Social_count  (int, count of how many times a social media platform is mentioned)\n",
    "- Know  (int, extracted from verb_lemma dictionary column)\n",
    "- Be  (int, extracted from verb_lemma dictionary column)\n",
    "- Do  (int, extracted from verb_lemma dictionary column)\n",
    "- Mean  (int, extracted from verb_lemma dictionary column)\n",
    "- Make  (int, extracted from verb_lemma dictionary column)\n",
    "- Go  (int, extracted from verb_lemma dictionary column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Genre-Topic</th>\n",
       "      <th>Scripted/Un</th>\n",
       "      <th>Fiction/Non</th>\n",
       "      <th>Format</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Opinion_count</th>\n",
       "      <th>Prep_per_sent</th>\n",
       "      <th>Donation_appeal</th>\n",
       "      <th>Social</th>\n",
       "      <th>Know</th>\n",
       "      <th>Be</th>\n",
       "      <th>Do</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Make</th>\n",
       "      <th>Go</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNIDENTIFIED PERSON #1: Listener-supported WN...</td>\n",
       "      <td>The Great Vaccinator</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>1.173352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.493</td>\n",
       "      <td>9.743</td>\n",
       "      <td>5.790</td>\n",
       "      <td>1.562</td>\n",
       "      <td>2.206</td>\n",
       "      <td>4.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAD: Hey, Iâm Jad Abumrad.    ROBERT: Iâm Rob...</td>\n",
       "      <td>Poop Train</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>1.076316</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.909</td>\n",
       "      <td>4.545</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SONGS THAT CROSS BORDERS FINAL WEB TRANSCRIPT...</td>\n",
       "      <td>Songs that Cross Borders</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.917411</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.694</td>\n",
       "      <td>11.111</td>\n",
       "      <td>2.189</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wonderful</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[comedy, society]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.9</td>\n",
       "      <td>152</td>\n",
       "      <td>Wonderful! 152: Air Milk \\nPublished September...</td>\n",
       "      <td>air milk</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>1.020260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.306</td>\n",
       "      <td>12.672</td>\n",
       "      <td>2.847</td>\n",
       "      <td>1.286</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Welcome to Nightvale</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[comedy, sci-fi]</td>\n",
       "      <td>scripted</td>\n",
       "      <td>fiction</td>\n",
       "      <td>news</td>\n",
       "      <td>4.8</td>\n",
       "      <td>144</td>\n",
       "      <td>It's turtles all the way down, but, man, it's ...</td>\n",
       "      <td>the dreamer</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.644</td>\n",
       "      <td>4.932</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>654</td>\n",
       "      <td>Prologue      Sean Cole   From WBEZ Chicago,...</td>\n",
       "      <td>The Feather Heist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>1.491250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.181</td>\n",
       "      <td>8.217</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.590</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JA: Jad Abumrad  SA: Simon Adler  AM: Annie M...</td>\n",
       "      <td>The Curious Case of the Russian Flash Mob ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>2.365759</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.850</td>\n",
       "      <td>6.502</td>\n",
       "      <td>3.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.651</td>\n",
       "      <td>2.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This American Life</th>\n",
       "      <td>1.5</td>\n",
       "      <td>[society, history]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.6</td>\n",
       "      <td>483</td>\n",
       "      <td>Prologue      Ira Glass   I spoke with Julia...</td>\n",
       "      <td>Self-Improvement Kick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>1.264352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.959</td>\n",
       "      <td>8.541</td>\n",
       "      <td>3.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.555</td>\n",
       "      <td>3.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td>152</td>\n",
       "      <td>Note: This show periodically replaces their ad...</td>\n",
       "      <td>final draft</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>1.073831</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.186</td>\n",
       "      <td>7.600</td>\n",
       "      <td>2.967</td>\n",
       "      <td>1.458</td>\n",
       "      <td>2.186</td>\n",
       "      <td>2.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiolab</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[society, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>storytelling</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jim Dixon: Hello there. This is Jim Dickson s...</td>\n",
       "      <td>An Ice-Cold Case</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>1.156550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.535</td>\n",
       "      <td>8.065</td>\n",
       "      <td>2.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Hosts           Genre-Topic Scripted/Un Fiction/Non  \\\n",
       "podcast                                                                     \n",
       "Radiolab                2.0  [society, education]  unscripted  nonfiction   \n",
       "Radiolab                2.0  [society, education]  unscripted  nonfiction   \n",
       "Radiolab                2.0  [society, education]  unscripted  nonfiction   \n",
       "Wonderful               2.0     [comedy, society]  unscripted  nonfiction   \n",
       "Welcome to Nightvale    1.0      [comedy, sci-fi]    scripted     fiction   \n",
       "This American Life      1.5    [society, history]  unscripted  nonfiction   \n",
       "Radiolab                2.0  [society, education]  unscripted  nonfiction   \n",
       "This American Life      1.5    [society, history]  unscripted  nonfiction   \n",
       "Friendly Fire           3.0     [history, movies]  unscripted  nonfiction   \n",
       "Radiolab                2.0  [society, education]  unscripted  nonfiction   \n",
       "\n",
       "                            Format  Rating Episode  \\\n",
       "podcast                                              \n",
       "Radiolab              storytelling     4.7     NaN   \n",
       "Radiolab              storytelling     4.7     NaN   \n",
       "Radiolab              storytelling     4.7     NaN   \n",
       "Wonderful                     chat     4.9     152   \n",
       "Welcome to Nightvale          news     4.8     144   \n",
       "This American Life    storytelling     4.6     654   \n",
       "Radiolab              storytelling     4.7     NaN   \n",
       "This American Life    storytelling     4.6     483   \n",
       "Friendly Fire                recap     4.6     152   \n",
       "Radiolab              storytelling     4.7     NaN   \n",
       "\n",
       "                                                                   Text  \\\n",
       "podcast                                                                   \n",
       "Radiolab               UNIDENTIFIED PERSON #1: Listener-supported WN...   \n",
       "Radiolab               JAD: Hey, Iâm Jad Abumrad.    ROBERT: Iâm Rob...   \n",
       "Radiolab               SONGS THAT CROSS BORDERS FINAL WEB TRANSCRIPT...   \n",
       "Wonderful             Wonderful! 152: Air Milk \\nPublished September...   \n",
       "Welcome to Nightvale  It's turtles all the way down, but, man, it's ...   \n",
       "This American Life      Prologue      Sean Cole   From WBEZ Chicago,...   \n",
       "Radiolab               JA: Jad Abumrad  SA: Simon Adler  AM: Annie M...   \n",
       "This American Life      Prologue      Ira Glass   I spoke with Julia...   \n",
       "Friendly Fire         Note: This show periodically replaces their ad...   \n",
       "Radiolab               Jim Dixon: Hello there. This is Jim Dickson s...   \n",
       "\n",
       "                                                                  Title  Year  \\\n",
       "podcast                                                                         \n",
       "Radiolab                                           The Great Vaccinator  2020   \n",
       "Radiolab                                                     Poop Train  2013   \n",
       "Radiolab                                       Songs that Cross Borders  2019   \n",
       "Wonderful                                                      air milk  2020   \n",
       "Welcome to Nightvale                                        the dreamer  2019   \n",
       "This American Life                                    The Feather Heist   NaN   \n",
       "Radiolab                  The Curious Case of the Russian Flash Mob ...  2018   \n",
       "This American Life                               Self-Improvement Kick    NaN   \n",
       "Friendly Fire                                               final draft         \n",
       "Radiolab                                               An Ice-Cold Case  2013   \n",
       "\n",
       "                      ... Opinion_count Prep_per_sent  Donation_appeal Social  \\\n",
       "podcast               ...                                                       \n",
       "Radiolab              ...      0.004596      1.173352                0      0   \n",
       "Radiolab              ...      0.005455      1.076316                0      0   \n",
       "Radiolab              ...      0.001684      0.917411                0      0   \n",
       "Wonderful             ...      0.013774      1.020260                0      0   \n",
       "Welcome to Nightvale  ...      0.005479      1.437500                0      0   \n",
       "This American Life    ...      0.003976      1.491250                0      0   \n",
       "Radiolab              ...      0.009288      2.365759                0      0   \n",
       "This American Life    ...      0.005380      1.264352                0      0   \n",
       "Friendly Fire         ...      0.013535      1.073831                0      0   \n",
       "Radiolab              ...      0.004608      1.156550                0      0   \n",
       "\n",
       "                       Know      Be     Do   Mean   Make     Go  \n",
       "podcast                                                          \n",
       "Radiolab              3.493   9.743  5.790  1.562  2.206  4.963  \n",
       "Radiolab              2.909   4.545  4.000  0.000  0.000  4.909  \n",
       "Radiolab              2.694  11.111  2.189  0.842  0.000  4.377  \n",
       "Wonderful             3.306  12.672  2.847  1.286  2.571  2.847  \n",
       "Welcome to Nightvale  1.644   4.932  1.096  0.000  0.000  1.644  \n",
       "This American Life    3.181   8.217  1.723  1.590  0.000  2.651  \n",
       "Radiolab              4.850   6.502  3.922  0.000  1.651  2.374  \n",
       "This American Life    2.959   8.541  3.430  0.000  2.555  3.093  \n",
       "Friendly Fire         2.186   7.600  2.967  1.458  2.186  2.551  \n",
       "Radiolab              2.535   8.065  2.304  0.000  0.000  1.382  \n",
       "\n",
       "[10 rows x 77 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate columns by what kind of model they can fit.  Target features, numerical features, and lexical features will be three separate csvs.  Transfer csvs to crc and do machine learning there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out dataframe used for regression -- all numerical values\n",
    "num_df = podcast_df[['Hosts','Rating','Token_count','Avg_token_len', 'Avg_sent_len', 'TTR','Avg_kband',\n",
    "                     'Noun_freq','Proper_noun_freq','Verb_freq','Adj_freq','Adv_freq',\n",
    "                     'Interjection_freq', 'Preposition_freq', 'Conjunction_freq', 'Avg_noun_len',\n",
    "                     'Avg_verb_len', 'Avg_adj_len', 'Avg_adv_len', 'i_count', 'you_count', 'she_count',\n",
    "                     'he_count', 'it_count', 'they_count', 'we_count', 'Know','Be', 'Do', 'Mean',\n",
    "                     'Make', 'Go', 'Organization', 'Art', 'Date', 'Geopolitical', 'Numbers', 'Event',\n",
    "                     'Cash', 'Time', 'Product', 'period_freq', 'excl_freq', 'quest_freq', 'hyph_freq',\n",
    "                     'Tag_len', 'Swear_count', 'Fake_swear_count', 'Opinion_count', 'Prep_per_sent',\n",
    "                     'Donation_appeal', 'Social']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hosts</th>\n",
       "      <th>Genre-Topic</th>\n",
       "      <th>Scripted/Un</th>\n",
       "      <th>Fiction/Non</th>\n",
       "      <th>Format</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[history, movies]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>recap</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[history, education]</td>\n",
       "      <td>unscripted</td>\n",
       "      <td>nonfiction</td>\n",
       "      <td>chat</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1436 rows Ã 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Hosts           Genre-Topic Scripted/Un Fiction/Non  \\\n",
       "podcast                                                                   \n",
       "Friendly Fire         3.0     [history, movies]  unscripted  nonfiction   \n",
       "Friendly Fire         3.0     [history, movies]  unscripted  nonfiction   \n",
       "Friendly Fire         3.0     [history, movies]  unscripted  nonfiction   \n",
       "Friendly Fire         3.0     [history, movies]  unscripted  nonfiction   \n",
       "Friendly Fire         3.0     [history, movies]  unscripted  nonfiction   \n",
       "...                   ...                   ...         ...         ...   \n",
       "You're Wrong About    2.0  [history, education]  unscripted  nonfiction   \n",
       "You're Wrong About    2.0  [history, education]  unscripted  nonfiction   \n",
       "You're Wrong About    2.0  [history, education]  unscripted  nonfiction   \n",
       "You're Wrong About    2.0  [history, education]  unscripted  nonfiction   \n",
       "You're Wrong About    2.0  [history, education]  unscripted  nonfiction   \n",
       "\n",
       "                   Format  Rating  Year  \n",
       "podcast                                  \n",
       "Friendly Fire       recap     4.6        \n",
       "Friendly Fire       recap     4.6  1951  \n",
       "Friendly Fire       recap     4.6        \n",
       "Friendly Fire       recap     4.6  1970  \n",
       "Friendly Fire       recap     4.6  1942  \n",
       "...                   ...     ...   ...  \n",
       "You're Wrong About   chat     4.6  2021  \n",
       "You're Wrong About   chat     4.6  2021  \n",
       "You're Wrong About   chat     4.6  2021  \n",
       "You're Wrong About   chat     4.6  2021  \n",
       "You're Wrong About   chat     4.6  2021  \n",
       "\n",
       "[1436 rows x 7 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate target df\n",
    "target_df = podcast_df[['Hosts', 'Genre-Topic', 'Scripted/Un', 'Fiction/Non', 'Format', 'Rating', 'Year']]\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-c408793950ac>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Tag1'] = target_df['Genre-Topic'].map(lambda x: x[0])\n",
      "<ipython-input-86-c408793950ac>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Tag2'] = target_df['Genre-Topic'].map(lambda x: x[-1])\n"
     ]
    }
   ],
   "source": [
    "# make tag 1 and tag 2 their own columns, rather than a list in a single column\n",
    "target_df['Tag1'] = target_df['Genre-Topic'].map(lambda x: x[0])\n",
    "target_df['Tag2'] = target_df['Genre-Topic'].map(lambda x: x[-1])\n",
    "target_df = target_df.drop(['Genre-Topic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Top50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>podcast</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>final draft</td>\n",
       "      <td>(  \\n, Note, :, This, show, periodically, repl...</td>\n",
       "      <td>[(the, 484), (a, 431), (Host, 402), (I, 371), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>final draft</td>\n",
       "      <td>(  \\n, Note, :, This, show, periodically, repl...</td>\n",
       "      <td>[(the, 481), (Host, 437), (a, 306), (I, 261), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>final draft</td>\n",
       "      <td>(  \\n, Note, :, This, show, periodically, repl...</td>\n",
       "      <td>[(the, 536), (that, 400), (a, 392), (Host, 382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>final draft</td>\n",
       "      <td>(  \\n, Note, :, This, show, periodically, repl...</td>\n",
       "      <td>[(the, 461), (a, 300), (Host, 282), (that, 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friendly Fire</th>\n",
       "      <td>final draft</td>\n",
       "      <td>(  \\n, Note, :, This, show, periodically, repl...</td>\n",
       "      <td>[(the, 505), (Host, 355), (a, 321), (that, 301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>Vanessa Williams Part 2: Saving The Best For Last</td>\n",
       "      <td>(Sarah, :,   , The, point, of, shaming, someon...</td>\n",
       "      <td>[(to, 387), (of, 343), (the, 329), (that, 312)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>The O.J. Simpson Trial: From the Mixed-Up File...</td>\n",
       "      <td>(Sarah, :,   , Economically, like, it, was, on...</td>\n",
       "      <td>[(the, 309), (of, 265), (to, 260), (like, 258)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>Bonus: \"The Dark Knight\"</td>\n",
       "      <td>(Mike,  , :, Ooh, ,, I, have, one, ,, I, have,...</td>\n",
       "      <td>[(the, 523), (I, 458), (like, 438), (to, 374),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>\"Political Correctness\"</td>\n",
       "      <td>(Also, ,, is, free, speech, a, right, in, the,...</td>\n",
       "      <td>[(the, 522), (of, 463), (to, 382), (a, 348), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You're Wrong About</th>\n",
       "      <td>Vanessa Williams Part 1: Becoming Miss America</td>\n",
       "      <td>(Sarah, Marshall, :,  , Oh, ,, my, God, ., It,...</td>\n",
       "      <td>[(the, 535), (like, 480), (to, 397), (of, 387)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1436 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Title  \\\n",
       "podcast                                                                 \n",
       "Friendly Fire                                             final draft   \n",
       "Friendly Fire                                             final draft   \n",
       "Friendly Fire                                             final draft   \n",
       "Friendly Fire                                             final draft   \n",
       "Friendly Fire                                             final draft   \n",
       "...                                                               ...   \n",
       "You're Wrong About  Vanessa Williams Part 2: Saving The Best For Last   \n",
       "You're Wrong About  The O.J. Simpson Trial: From the Mixed-Up File...   \n",
       "You're Wrong About                          Bonus: \"The Dark Knight\"    \n",
       "You're Wrong About                            \"Political Correctness\"   \n",
       "You're Wrong About     Vanessa Williams Part 1: Becoming Miss America   \n",
       "\n",
       "                                                               Tokens  \\\n",
       "podcast                                                                 \n",
       "Friendly Fire       (  \\n, Note, :, This, show, periodically, repl...   \n",
       "Friendly Fire       (  \\n, Note, :, This, show, periodically, repl...   \n",
       "Friendly Fire       (  \\n, Note, :, This, show, periodically, repl...   \n",
       "Friendly Fire       (  \\n, Note, :, This, show, periodically, repl...   \n",
       "Friendly Fire       (  \\n, Note, :, This, show, periodically, repl...   \n",
       "...                                                               ...   \n",
       "You're Wrong About  (Sarah, :,   , The, point, of, shaming, someon...   \n",
       "You're Wrong About  (Sarah, :,   , Economically, like, it, was, on...   \n",
       "You're Wrong About  (Mike,  , :, Ooh, ,, I, have, one, ,, I, have,...   \n",
       "You're Wrong About  (Also, ,, is, free, speech, a, right, in, the,...   \n",
       "You're Wrong About  (Sarah, Marshall, :,  , Oh, ,, my, God, ., It,...   \n",
       "\n",
       "                                                                Top50  \n",
       "podcast                                                                \n",
       "Friendly Fire       [(the, 484), (a, 431), (Host, 402), (I, 371), ...  \n",
       "Friendly Fire       [(the, 481), (Host, 437), (a, 306), (I, 261), ...  \n",
       "Friendly Fire       [(the, 536), (that, 400), (a, 392), (Host, 382...  \n",
       "Friendly Fire       [(the, 461), (a, 300), (Host, 282), (that, 253...  \n",
       "Friendly Fire       [(the, 505), (Host, 355), (a, 321), (that, 301...  \n",
       "...                                                               ...  \n",
       "You're Wrong About  [(to, 387), (of, 343), (the, 329), (that, 312)...  \n",
       "You're Wrong About  [(the, 309), (of, 265), (to, 260), (like, 258)...  \n",
       "You're Wrong About  [(the, 523), (I, 458), (like, 438), (to, 374),...  \n",
       "You're Wrong About  [(the, 522), (of, 463), (to, 382), (a, 348), (...  \n",
       "You're Wrong About  [(the, 535), (like, 480), (to, 397), (of, 387)...  \n",
       "\n",
       "[1436 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podtoks_df = podcast_df[['Title', 'Tokens', 'Top50']]\n",
    "podtoks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "podtoks_df.to_csv('data/podtoks_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv('data/target_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.to_csv('data/num_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
