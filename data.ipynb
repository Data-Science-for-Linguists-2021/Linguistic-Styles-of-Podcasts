{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load elements and build DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import collections\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import statistics\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spacy objects\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r Nightvale_df\n",
    "%store -r myDNA_df\n",
    "%store -r YWA_df\n",
    "%store -r uu_df\n",
    "%store -r radiolab_df\n",
    "%store -r tal_df\n",
    "%store -r bullseye\n",
    "%store -r mother\n",
    "%store -r hodgman\n",
    "%store -r flophouse\n",
    "%store -r switchblade\n",
    "%store -r mbmbam\n",
    "%store -r sawbones\n",
    "%store -r wonderful\n",
    "%store -r tgg\n",
    "%store -r ffire\n",
    "%store -r shmanners\n",
    "%store -r taz\n",
    "\n",
    "\n",
    "\n",
    "# %store -r freak_df\n",
    "# %store -r Lore_df\n",
    "# %store -r Invisible_df\n",
    "# %store -r OnBeing_df\n",
    "# %store -r StoryCorps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a multindexed dataframe\n",
    "data = pd.concat([Nightvale_df.reset_index(drop=True),\n",
    "                  myDNA_df.reset_index(drop=True),\n",
    "                  YWA_df.reset_index(drop=True),                  \n",
    "                  uu_df.reset_index(drop=True),\n",
    "                  radiolab_df.reset_index(drop=True),\n",
    "                  tal_df.reset_index(drop=True),\n",
    "                  bullseye,\n",
    "                  mother,\n",
    "                  hodgman,\n",
    "                  flophouse,\n",
    "                  switchblade,\n",
    "                  mbmbam,\n",
    "                  sawbones,\n",
    "                  wonderful,\n",
    "                  tgg,\n",
    "                  ffire,\n",
    "                  shmanners,\n",
    "                  taz],\n",
    "                  # StoryCorps_df, OnBeing_df, Invisible_df, Lore_df],\n",
    "                  keys = ['Nightvale','Move Your DNA','You\\'re Wrong About','Unlocking Us',\n",
    "                         'Radiolab','This American Life', 'Bullseye with Jesse Thorn','One Bad Mother',\n",
    "                         'Judge John Hodgman','The Flophouse','Switchblade Sisters',\n",
    "                         'MBMBaM','Sawbones','Wonderful','The Greatest Generation','Friendly Fire','Shmanners',\n",
    "                          'The Adventure Zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Podcast'])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Text[0][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anticipating a log of ugly floats\n",
    "def percent(decimal):\n",
    "    decimal *= 100\n",
    "    percentage = '{:.3f}'.format(decimal)\n",
    "    percentage = float(percentage)\n",
    "    return percentage\n",
    "\n",
    "percent(0.45615981981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Columns: Tokens, Top50, Token_count, Avg_token_len, TTR, kband, Bigrams, Bigram_top25, Sent_toks, Avg_sent_len, POS_frequency, POS_length, Verb_lemmas, Ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "data['Tokens'] = data.Text.map(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top50(Tokens):\n",
    "    counts = Counter(t.text for t in Tokens if t.is_alpha)\n",
    "    return counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Top50'] = data.Tokens.map(top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Token_count'] = data.Tokens.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_len(Tokens):\n",
    "    if len(Tokens) > 10:\n",
    "        lengths = [(w, len(w.text)) for w in Tokens if w.is_alpha]\n",
    "    else:\n",
    "        lengths = [('null',0)]\n",
    "    \n",
    "    avg = statistics.mean([l[-1] for l in lengths])\n",
    "    \n",
    "    return lengths, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Token_lengths'] = data.Tokens.map(lambda x: word_len(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Avg_token_len'] = data.Tokens.map(lambda x: word_len(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTR\n",
    "def get_ttr(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        lower = [t.text.lower() for t in Tokens if t.is_alpha]\n",
    "        ttr = percent(len(set(lower))/len(lower))\n",
    "    else:\n",
    "        ttr = 0\n",
    "        \n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TTR'] = data.Tokens.map(get_ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import google kbands\n",
    "f = open('data/goog_kband.pkl','rb')\n",
    "goog_kband = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "goog_kband['throughout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kband(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        kbands = []\n",
    "        for t in Tokens:\n",
    "            if t.lemma_ in goog_kband:\n",
    "                kbands.append((t, goog_kband[t.lemma_]))\n",
    "        avg_kband = statistics.mean([t[1] for t in kbands])\n",
    "    else:\n",
    "        kbands = 0\n",
    "        avg_kband = 0\n",
    "    \n",
    "    return kbands, avg_kband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['kband'] = data.Tokens.map(lambda x: get_kband(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Avg_kband'] = data.Tokens.map(lambda x: get_kband(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(Tokens):\n",
    "    if len(Tokens) > 1:\n",
    "        bigrams = []\n",
    "        for t in Tokens[:-1]:\n",
    "            print(t)\n",
    "            if t.text.isalpha() and Tokens[t.i + 1].text.isalpha():\n",
    "                bigram = (t.text.lower(), Tokens[t.i + 1].text.lower())\n",
    "                bigrams.append(bigram)\n",
    "        counts = Counter(b for b in bigrams).most_common(25)\n",
    "    else:\n",
    "        bigrams = 'null'\n",
    "        \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add bigrams column\n",
    "data['Bigrams'] = data.Tokens.map(lambda x: bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist bigrams\n",
    "data['Bigram_top25'] = data.Bigrams.map(lambda x: Counter(x).most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS'] = data.Tokens.map(lambda t: [(w, w.pos_) for w in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighs pos frequency against total text length\n",
    "def POS_frequency(POS_text):\n",
    "    counts = Counter(elem[-1].upper() for elem in POS_text)\n",
    "    total = len(POS_text)\n",
    "    \n",
    "    pos_freq = {}\n",
    "    for (pos, count) in counts.items():\n",
    "        pos_freq[pos] = percent(count/total)\n",
    "        \n",
    "    return pos_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS_freq'] = data.POS.map(POS_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.POS_freq[0]['NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Noun_freq'] = data.POS_freq.map(lambda x: x.get('NOUN', 'null'))\n",
    "data['Verb_freq'] = data.POS_freq.map(lambda x: x.get('VERB', 'null'))\n",
    "data['Adj_freq'] = data.POS_freq.map(lambda x: x.get('ADJ', 'null'))\n",
    "data['Adv_freq'] = data.POS_freq.map(lambda x: x.get('ADV', 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_length(POS_text):\n",
    "    pos_dict = {'NOUN': 0, 'VERB': 0, 'ADV': 0, 'ADJ': 0}\n",
    "    pron_dict = {'i': 0, 'you': 0, 'she': 0, 'he': 0, 'it': 0, 'they': 0, 'we': 0}\n",
    "    for (token, pos) in POS_text:\n",
    "        if pos in pos_dict.keys():\n",
    "            pos_dict[pos] = (pos_dict[pos] + len(token.text))/2\n",
    "        if token.text in pron_dict.keys():\n",
    "            pron_dict[token.text] = pron_dict[token.text] + 1\n",
    "    \n",
    "    if sum(pron_dict.values()) != 0:\n",
    "        pron_total = sum(pron_dict.values())\n",
    "    \n",
    "    if sum(pron_dict.values()) != 0:\n",
    "        for (p, c) in pron_dict.items():\n",
    "            pron_dict[p] = percent(c/pron_total)\n",
    "    \n",
    "    \n",
    "    return pos_dict, pron_dict\n",
    "\n",
    "# Average word length of each POS\n",
    "# POS_length[0][0] = noun\n",
    "#           [0][1] = verb\n",
    "#           [0][2] = adv\n",
    "#           [0][3] = adj\n",
    "\n",
    "# Individual pronoun occurrence weighed against total # of pronouns\n",
    "# POS_length[1][1] = 'i'\n",
    "#           [1][2] = 'you'\n",
    "#           [1][3] ='she'\n",
    "#           [1][4] = 'he'\n",
    "#           [1][5] = 'it'\n",
    "#           [1][6] = 'they'\n",
    "#           [1][7] = 'we'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS_length'] = data.POS.map(lambda p: POS_length(p)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Avg_noun_len'] = data.POS_length.map(lambda d: d['NOUN'])\n",
    "data['Avg_verb_len'] = data.POS_length.map(lambda d: d['VERB'])\n",
    "data['Avg_adj_len'] = data.POS_length.map(lambda d: d['ADJ'])\n",
    "data['Avg_adv_len'] = data.POS_length.map(lambda d: d['ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighs occurrence of each ent against total text length\n",
    "def ent_counter(Tokens):\n",
    "    counts = Counter(elem.label_ for elem in Tokens.ents)\n",
    "    # print(sum(counts.values()))\n",
    "    \n",
    "    ent_counter = {}\n",
    "    for (ent, value) in counts.items():\n",
    "        ent_counter[ent] = percent(value/len(Tokens))\n",
    "    # print(sum(ent_counter.values()))\n",
    "    \n",
    "    return ent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOU ARE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common verb lemmas\n",
    "def verb_lemmas(POS_text):\n",
    "    counts = Counter(elem[0].lemma_ for elem in POS_text if elem[1] == 'VERB')\n",
    "    \n",
    "    verb_counter = {}\n",
    "    for (verb, value) in counts.most_common(20):\n",
    "        verb_counter[verb] = percent(value/sum(counts.values()))\n",
    "        \n",
    "    return verb_counter\n",
    "\n",
    "verb_lemmas(data.POS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sent_toks'] = data.Text.map(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor alteration to unit_len\n",
    "def sent_len(doc):\n",
    "    sentlens = []\n",
    "    for c in doc:\n",
    "        length = len([l for l in c.split()])\n",
    "        sentlens.append((c, length))\n",
    "        \n",
    "    return sentlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sent_length'] = data.Sent_toks.map(sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to extract host names??  Some are full names and some are just first names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['The Flophouse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
