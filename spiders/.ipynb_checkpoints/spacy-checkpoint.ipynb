{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy\n",
    "04-08-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r Nightvale_df\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the English library from spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize nlp obect\n",
    "nlp = English()\n",
    "\n",
    "# nlp object analyzes text\n",
    "# - contains proccesing pipeline\n",
    "# - includes language-specific rules for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you pass a string of text through the nlp object,\n",
    "#     it becomes a \"document\" object\n",
    "doc = nlp(Nightvale_df.Text[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CECIL\n",
      ":\n",
      "As\n",
      "a\n",
      "matter\n",
      "of\n",
      "fact\n",
      ",\n",
      "the\n",
      "facts\n",
      "do\n",
      "nâ€™t\n",
      "matter\n",
      ".\n",
      "Welcome\n",
      "to\n",
      "Night\n",
      "Vale\n",
      " \n",
      "Listeners\n",
      ",\n",
      "it\n",
      "is\n",
      "upon\n",
      "us\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing\n",
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is upon\n"
     ]
    }
   ],
   "source": [
    "# span\n",
    "# spanning DOESN'T create new information, it's just a view of the doc\n",
    "\n",
    "span = doc[-4:-2]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:      [0, 1, 2, 3, 4]\n",
      "Text:       ['It', 'costs', '$', '5', '.']\n",
      "is_alpha:   [True, True, False, False, False]\n",
      "is_punct:   [False, False, False, False, True]\n",
      "like_num:   [(It, False), (costs, False), ($, False), (5, True), (., False)]\n"
     ]
    }
   ],
   "source": [
    "# token attributes (aka lexical attributes)\n",
    "\n",
    "doc = nlp('It costs $5.')\n",
    "print('Index:     ', [token.i for token in doc])\n",
    "print('Text:      ', [token.text for token in doc])\n",
    "print('is_alpha:  ', [token.is_alpha for token in doc])\n",
    "print('is_punct:  ', [token.is_punct for token in doc])\n",
    "print('like_num:  ', [(token, token.like_num) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# \"small english model\"\n",
    "# en_core_web_sm is a nlp object trained on lots of language data\n",
    "#     that can predict part of speech and other language metadata\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'She ate the pizza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "She PRON nsubj ate\n",
      "ate VERB\n",
      "ate VERB ROOT ate\n",
      "the DET\n",
      "the DET det pizza\n",
      "pizza NOUN\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    # syntactic dependency info\n",
    "    \n",
    "# attributes that return strings end with an underscore.  \n",
    "# attributes that return IDs end without an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = u'Apple is looking at buying a U.K. startup for $1 billion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K. GPE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1 billion MONEY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Monetary values, including unit'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(string)\n",
    "\n",
    "# named entities (proper nouns)\n",
    "# returns an iterator of span objects\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    spacy.explain(ent.label_) # spacy.explain('label')!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = u'Why does Aidan insist on chewing on his sleeves 24/7?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why         ADV       advmod    \n",
      "does        AUX       aux       \n",
      "Aidan       PROPN     nsubj     \n",
      "insist      VERB      ROOT      \n",
      "on          ADP       prep      \n",
      "chewing     VERB      pcomp     \n",
      "on          ADP       prep      \n",
      "his         PRON      poss      \n",
      "sleeves     NOUN      pobj      \n",
      "24/7        NUM       appos     \n",
      "?           PUNCT     punct     \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(string)\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/7\n",
      "24/7 CARDINAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Numerals that do not fall under another type'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and its label\n",
    "    print(ent)\n",
    "    print(ent.text, ent.label_)\n",
    "    spacy.explain(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entity: Aidan\n"
     ]
    }
   ],
   "source": [
    "# the model didn't predict \"Aidan\".  Manually add it\n",
    "name = doc[2:3]\n",
    "print('Missing entity: {}'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and initialize matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_pattern(string):\n",
    "    \n",
    "    doc = nlp(string)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    patterns = []\n",
    "    \n",
    "    add_items = True\n",
    "    add_dict = True    \n",
    "    \n",
    "    while add_dict:        \n",
    "        match_single = {}                    \n",
    "            \n",
    "        while add_items:       \n",
    "            tag = input('enter tag(lowercase):  ').upper()\n",
    "            if tag == '0':\n",
    "                add_dict = False\n",
    "                break\n",
    "\n",
    "            string = input('enter string:  ')\n",
    "            if string == 'true' or string == 'false':\n",
    "                string = bool(string)\n",
    "\n",
    "            if tag == 'POS':\n",
    "                string = string.upper()\n",
    "            \n",
    "            match_single[tag] = string\n",
    "            \n",
    "            add_items = input('add more to this dict?(y/n)  ')\n",
    "            if add_items == 'n':\n",
    "                patterns.append(match_single)\n",
    "                break\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if len(patterns) == 0:\n",
    "        return\n",
    "        \n",
    "    for p in patterns:\n",
    "        print(p)\n",
    "   \n",
    "    matcher.add('pattern', [patterns])\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    \n",
    "    match_strings = []\n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        match_strings.append(matched_span)\n",
    "    print('{} matches found'.format(len(match_strings)))\n",
    "    return match_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  pos\n",
      "enter string:  det\n",
      "add more to this dict?(y/n)  n\n",
      "enter tag(lowercase):  pos\n",
      "enter string:  noun\n",
      "add more to this dict?(y/n)  n\n",
      "enter tag(lowercase):  pos\n",
      "enter string:  verb\n",
      "add more to this dict?(y/n)  n\n",
      "enter tag(lowercase):  0\n",
      "{'POS': 'DET'}\n",
      "{'POS': 'NOUN'}\n",
      "{'POS': 'VERB'}\n",
      "1 matches found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[The dog went]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_pattern('The dog went there.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matcher to look for specific tokens\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "matcher.add('FIFA_WC', [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('2018 FIFA World Cup: France won!')\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'LEMMA':'love', 'POS':'VERB'},\n",
    "    {'POS':'NOUN'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I loved dogs but now I love cats more.')\n",
    "    \n",
    "# the matcher is more inclusive than regex because it will include \n",
    "#      words with the same root but different affixes -- i.e. 'loved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operators and quanitifiers\n",
    "\n",
    "pattern = [\n",
    "    {'LEMMA':'buy'},\n",
    "    {'POS':'DET','OP':'?'},\n",
    "    {'POS':'NOUN'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  0\n"
     ]
    }
   ],
   "source": [
    "basic_pattern('I bought a smartphone.  Now I\\'m buying apps.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "operators and quanitifers\n",
    "(include these in the dictionary of the desired match pattern)\n",
    "\n",
    "- {'OP':'!'} Negation - match 0 times\n",
    "- {'OP':'?'} Optional - match 0 or 1 times\n",
    "- {'OP':'+'} Match 1 or more times\n",
    "- {'OP':'*'} Match 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  0\n"
     ]
    }
   ],
   "source": [
    "string = \"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\"\n",
    "\n",
    "basic_pattern(string)\n",
    "# pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  0\n"
     ]
    }
   ],
   "source": [
    "string = 'Why won\\'t Aidan stop eating his sleeves??!!'\n",
    "\n",
    "basic_pattern(string)\n",
    "# {'POS': 'PROPN'} matches proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  0\n"
     ]
    }
   ],
   "source": [
    "string = 'Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.'\n",
    "\n",
    "basic_pattern(string)\n",
    "# returns an adjective followed by one or two nouns (one noun and one optional noun)\n",
    "# {'POS': 'ADJ'}\n",
    "# {'POS': 'NOUN'}\n",
    "# {'POS': 'NOUN', 'OP': '?'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter tag(lowercase):  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c8f57a7b9c23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbasic_pattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I have a Golden Retriever'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "basic_pattern('I have a Golden Retriever')[0].span.root.head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-eed5afb53033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\matcher\\phrasematcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.phrasematcher.PhraseMatcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "# phrase matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp('Golden Retriever')\n",
    "\n",
    "pattern = [\n",
    "    {'LOWER':'golden'},\n",
    "    {'LOWER':'retriever'}\n",
    "]\n",
    "\n",
    "matcher.add('dog', [pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared vocab and string storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('My kitties Leo and Piccolo are the cutest.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings are stored with a \"hash\" number.  spacy must \"see\" both to store it.  Lookup goes in both directions\n",
    "pickles_hash = nlp.vocab.strings['Piccolo']\n",
    "pickles_string = nlp.vocab.strings[pickles_hash]\n",
    "pickles_lexeme = nlp.vocab['Piccolo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Piccolo hash #: ', pickles_hash)\n",
    "print('Piccolo string: ', pickles_string)\n",
    "print('Piccolo lexeme\\'s text: ', pickles_lexeme.text)\n",
    "print('Piccolo lexeme\\'s orth: ', pickles_lexeme.orth)\n",
    "print('Is the Piccolo lexeme alpha? ', pickles_lexeme.is_alpha)\n",
    "print('Lexemes have the same attributes as tokens, except for POS tags, dependency info, or entity labels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Doc and Span objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# doc elements\n",
    "words = ['Hello','world','!']\n",
    "spaces = [True, False, False] # indicate whether the word is followed by a space\n",
    "\n",
    "# manually create doc from 3 arguments\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import doc and span classes\n",
    "from spacy.tokens import Doc, Span # need a doc to have a span!\n",
    "\n",
    "# create a span from a doc manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# add an optional label\n",
    "labeled_span = Span(doc, 1, 3, label='CURRENT_PLANET') # convention is to write label names in caps\n",
    "\n",
    "# add span to the doc.ents\n",
    "doc.ents = [labeled_span]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices when using Docs and Spans\n",
    "- Doc and span are mainly used to hold references and relationships of words and sentences\n",
    "    - convert result to string as LATE as possible\n",
    "    - use token attributes if possible (like token.i for the token's index)\n",
    "- Don't forget to pass in the shared vocab as the first argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Only', 'two', 'more', 'weeks', '!']\n",
    "spaces = [True, True, True, False, False]\n",
    "\n",
    "time_left = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "time_left\n",
    "time_left.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['I', 'like','Leo', 'the','Lion','and','Piccolo','the','Princess','.']\n",
    "spaces = [True, True, True, True, True, True, True, True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "leo = Span(doc, 2, 5, label='LEO')\n",
    "piccolo = Span(doc, 6, 9, label='PICCOLO')\n",
    "\n",
    "print(leo.text, leo.label_)\n",
    "print(piccolo.text, piccolo.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write my span into ents\n",
    "doc.ents = [leo, piccolo]\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (token, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Alex', 'likes','Leo', 'the','Lion','and','Piccolo','the','Princess','.']\n",
    "spaces = [True, True, True, True, True, True, True, True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy can compare two objects (of type token, doc, or span) and predict similarity with . . .\n",
    "    - Doc.similarity()\n",
    "    - Span.similarity()\n",
    "    - Token.similarity()\n",
    "    \n",
    "- return similarity score as float between 0 and 1\n",
    "\n",
    "- TO DO THIS, YOU MUST BE USING EITHER MEDIUM OR LARGE LANGUAGE MODELS because they contain word vector data\n",
    "    - CAN use: en_core_web_md (medium english model)\n",
    "    - CAN use: en_core_web_md (large english model)\n",
    "    - can NOT use: en_core_web_sm (small english model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "\n",
    "# order doesn't matter\n",
    "doc1.similarity(doc2)\n",
    "doc2.similarity(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I like pizza and pasta')\n",
    "token1 = doc[2] # pizza\n",
    "token2 = doc[4] # pasta\n",
    "\n",
    "token1.similarity(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare b/t different object types\n",
    "doc = nlp('I like pizza')\n",
    "token = nlp('soap')[0]\n",
    "doc.similarity(token)\n",
    "\n",
    "span = nlp('I like pizza and pasta')[2:5]  # pizza and pasta\n",
    "doc = nlp('McDonalds sells burgers')\n",
    "span.similarity(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize a doc\n",
    "doc = nlp('I have a banana')\n",
    "\n",
    "# access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_doc = nlp('I like cats')\n",
    "wrong_doc = nlp('I hate cats')\n",
    "\n",
    "right_doc.similarity(wrong_doc)\n",
    "\n",
    "# similarity scores are subjective!  These opposite statements are rated as very similar because they express sentiment\n",
    "#     about cats.  Keep in mind the objective of the program you're writing when it comes to similarity usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "print(span1)\n",
    "span2 = doc[-4:-1]\n",
    "print(span2)\n",
    "\n",
    "similarity = span1.similarity(span2)\n",
    "print('Similarity:', similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
